{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6s3nhj1LBq-"
   },
   "source": [
    "### **Content License Agreement**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : 본 자료는 삼성청년SW·AI아카데미의 컨텐츠 자산으로, 보안서약서에 의거하여 어떠한 사유로도 임의로 복사, 촬영, 녹음, 복제, 보관, 전송하거나 허가 받지 않은 저장매체를 이용한 보관, 제3자에게 누설, 공개 또는 사용하는 등의 무단 사용 및 불법 배포 시 법적 조치를 받을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjwW_TAkLFnH"
   },
   "source": [
    "# **Objectives**\n",
    "\n",
    "\n",
    "\n",
    "**1. 실습 개요**\n",
    "\n",
    "* **목표:** 학습된 `Pytorch` 모델을 온디바이스 배포용 `TFLite` 모델로 변환 및 최적화.\n",
    "* **핵심 과제:**\n",
    "    1.  **모델 재구성:** 표준 Hugging Face 모델을 `TFLite` 변환이 가능한 `ai-edge-torch` 라이브러리 기반 아키텍처로 재설계.\n",
    "    2.  **가중치 이식:** 원본 모델과 재구성된 모델 간 아키텍처 불일치를 해결하고, 가중치를 수동으로 매핑하는 커스텀 함수 구현.\n",
    "    3.  **성능 저하 해결:** 모델 재구성 과정에서 발생하는 미세 설정 차이를 분석하여 예측 성능 저하 문제 해결.\n",
    "\n",
    "\n",
    "\n",
    "**2. 실습 목적 및 배경**\n",
    "\n",
    "* **온디바이스 최적화 이해:** 표준 모델을 온디바이스용으로 재구성해야 하는 근본적인 이유 학습.\n",
    "* **아키텍처 분석 능력 함양:** 표준 트랜스포머와 온디바이스 최적화 모델(예: Fused QKV Layer)의 구조적 차이점 분석.\n",
    "* **온디바이스 AI 파이프라인 경험:** 모델 재구성, 가중치 이식, `TFLite` 변환, 최종 애플리케이션 배포까지 전 과정 이해.\n",
    "* **양자화(Quantization) 효과 이해:** 모델 경량화 및 추론 속도 향상, NPU 환경에서 `INT8` 연산의 성능 향상 원리 파악.\n",
    "\n",
    "\n",
    "**3. 획득 가능 역량**\n",
    "\n",
    "* **On-device AI 모델 최적화 역량:** 특정 하드웨어 환경에 맞게 모델을 재구성하고 양자화하여 성능을 극대화하는 실무 능력.\n",
    "* **엔드투엔드(End-to-End) 프로젝트 수행 역량:** 모델 선정부터 최적화, 변환, 배포까지 **On-device AI** 개발 파이프라인 전체를 완수하는 능력.\n",
    "\n",
    "\n",
    "\n",
    "**4. 실습 핵심 내용**\n",
    "\n",
    "* `SmolLM2` 모델 재구성 및 `TFLite` (LiteRT) 변환.\n",
    "* `TFLite Interpreter`를 이용한 추론 실행."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbJ7T8tuLK1P"
   },
   "source": [
    "# **Prerequistes**\n",
    "\n",
    "\n",
    "**[Prerequistes]**\n",
    "\n",
    "```bash\n",
    "ai_edge_torch: 0.4.0\n",
    "tensorflow: 2.19.0\n",
    "torch: 2.6.0+cu124\n",
    "torchvision: 0.21.0\n",
    "torchaudio: 2.6.0\n",
    "```\n",
    "\n",
    "\n",
    "**[install 명령어]**\n",
    "\n",
    "- colab에 ai_edge_torch 는 설치가 되어있지 않을 확률이 높기 때문에 해당 버전을 유의해서 설치해주세요.\n",
    "- tensorflow 및 torch 와의 버전이 맞아 떨어져야하기 때문에 두 개의 설치 명령어를 같이 실행해주세요.\n",
    "- 설치 도중 코랩에서 기본적으로 설치된 라이브러리와 새로 설치하는 라이브리리 사이에서 발생하는 의존성 문제가 발생할 수 있습니다. 이는 Python 패키지 관리 구조상, 특정 버전 조합이 완벽히 호환되지 않는 경우가 많기에 발생하고, 강의 실습에 큰 영향을 주지 않는 단순 Error이니 안심하고 실습을 진행해주셔도 됩니다.\n",
    "\n",
    "\n",
    "```bash\n",
    "! pip install tensorflow==2.19.0\n",
    "! pip install ai_edge_torch==0.4.0\n",
    "! pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cv7T44hrSPKB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "\n",
    "import ai_edge_torch\n",
    "print(ai_edge_torch.__version__)\n",
    "\n",
    "# 버전이 다르거나 설치되어있지 않다면 아래 명령어를 실행하도록 합니다.\n",
    "# ! pip install tensorflow==2.19.0\n",
    "# ! pip install ai_edge_torch==0.4.0\n",
    "# ! pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 351758,
     "status": "ok",
     "timestamp": 1756034717206,
     "user": {
      "displayName": "isu jeong",
      "userId": "03509833375949211012"
     },
     "user_tz": -540
    },
    "id": "VqKr-iNPmHIy",
    "outputId": "6a1f6343-1eca-4d6c-a5ed-c2ae8bce3d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.19.0 in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.19.0) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19.0) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0) (0.1.2)\n",
      "Collecting ai_edge_torch==0.4.0\n",
      "  Downloading ai_edge_torch-0.4.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ai_edge_torch==0.4.0) (2.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from ai_edge_torch==0.4.0) (1.16.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from ai_edge_torch==0.4.0) (0.6.2)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from ai_edge_torch==0.4.0) (0.9.0)\n",
      "Collecting torch<2.7.0,>=2.4.0 (from ai_edge_torch==0.4.0)\n",
      "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: tensorflow==2.19.* in /usr/local/lib/python3.12/dist-packages (from ai_edge_torch==0.4.0) (2.19.0)\n",
      "Collecting ai-edge-litert==1.2.* (from ai_edge_torch==0.4.0)\n",
      "  Downloading ai_edge_litert-1.2.0-cp312-cp312-manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting ai-edge-quantizer==0.1.* (from ai_edge_torch==0.4.0)\n",
      "  Downloading ai_edge_quantizer-0.1.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from ai_edge_torch==0.4.0) (0.5.3)\n",
      "Collecting torch-xla2>=0.0.1.dev20241201 (from torch-xla2[odml]>=0.0.1.dev20241201->ai_edge_torch==0.4.0)\n",
      "  Downloading torch_xla2-0.0.1.dev202412041639-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from ai-edge-litert==1.2.*->ai_edge_torch==0.4.0) (25.2.10)\n",
      "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/dist-packages (from ai-edge-quantizer==0.1.*->ai_edge_torch==0.4.0) (4.2.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (1.6.3)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (3.10.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.*->ai_edge_torch==0.4.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0) (3.19.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0) (1.3.0)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai_edge_torch==0.4.0) (8.4.1)\n",
      "Requirement already satisfied: jaxlib<=0.5.3,>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from jax->ai_edge_torch==0.4.0) (0.5.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.19.*->ai_edge_torch==0.4.0) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.*->ai_edge_torch==0.4.0) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.*->ai_edge_torch==0.4.0) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.*->ai_edge_torch==0.4.0) (0.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.*->ai_edge_torch==0.4.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.*->ai_edge_torch==0.4.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.*->ai_edge_torch==0.4.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.*->ai_edge_torch==0.4.0) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.*->ai_edge_torch==0.4.0) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.*->ai_edge_torch==0.4.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.*->ai_edge_torch==0.4.0) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<2.7.0,>=2.4.0->ai_edge_torch==0.4.0) (3.0.2)\n",
      "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai_edge_torch==0.4.0) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai_edge_torch==0.4.0) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest->torch-xla2>=0.0.1.dev20241201->torch-xla2[odml]>=0.0.1.dev20241201->ai_edge_torch==0.4.0) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.*->ai_edge_torch==0.4.0) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.*->ai_edge_torch==0.4.0) (0.1.2)\n",
      "Downloading ai_edge_torch-0.4.0-py3-none-any.whl (477 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m477.6/477.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ai_edge_litert-1.2.0-cp312-cp312-manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ai_edge_quantizer-0.1.0-py3-none-any.whl (153 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch_xla2-0.0.1.dev202412041639-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ai-edge-litert, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torch-xla2, ai-edge-quantizer, ai_edge_torch\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.4.0\n",
      "    Uninstalling triton-3.4.0:\n",
      "      Successfully uninstalled triton-3.4.0\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
      "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.8.0+cu126\n",
      "    Uninstalling torch-2.8.0+cu126:\n",
      "      Successfully uninstalled torch-2.8.0+cu126\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.6.0 which is incompatible.\n",
      "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed ai-edge-litert-1.2.0 ai-edge-quantizer-0.1.0 ai_edge_torch-0.4.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 torch-xla2-0.0.1.dev202412041639 triton-3.2.0\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.12/dist-packages (2.6.0)\n",
      "Collecting torchvision==0.21.0\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp312-cp312-linux_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio==2.6.0\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp312-cp312-linux_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (1.13.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.21.0) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.21.0) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.6.0) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision, torchaudio\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.23.0+cu126\n",
      "    Uninstalling torchvision-0.23.0+cu126:\n",
      "      Successfully uninstalled torchvision-0.23.0+cu126\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.8.0+cu126\n",
      "    Uninstalling torchaudio-2.8.0+cu126:\n",
      "      Successfully uninstalled torchaudio-2.8.0+cu126\n",
      "Successfully installed torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow==2.19.0\n",
    "! pip install ai_edge_torch==0.4.0\n",
    "! pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AAYmKoXLaSQ"
   },
   "source": [
    "\n",
    "# **Exercise Overview**\n",
    "\n",
    "### **들어가며: LiteRT (이전 TFLite)**\n",
    "LiteRT는 구글이 제공하는 온디바이스 AI 실행 엔진으로 원래는 TensorFlow Lite(TFLite)라는 이름으로 널리 쓰였는데, 2024년 이름이 Lite Runtime(LiteRT)으로 바뀌어졌습니다.\n",
    "\n",
    "단순히 이름만 바뀐 게 아니라, 앞으로는 TensorFlow 프레임워크에 종속되지 않고 PyTorch, JAX, Keras 등 다양한 프레임워크에서 만든 모델을 실행할 수 있는 범용 런타임으로 확장하려는 의도가 담겨 있습니다.\n",
    "\n",
    "주요 특징으로는 다음이 있습니다.\n",
    "\n",
    "- 온디바이스 AI 실행\n",
    "  - 모바일(Android, iOS), IoT, 임베디드 기기에서 AI 모델을 가볍게 실행할 수 있음.\n",
    "  - 클라우드 서버 없이 동작 → 지연시간 감소, 프라이버시 강화, 네트워크 의존도 줄어듦.\n",
    "\n",
    "- 다양한 프레임워크 지원\n",
    "  - 과거 TFLite는 TensorFlow에서 변환한 모델만 잘 지원했음.\n",
    "  - LiteRT는 PyTorch, JAX, Hugging Face 모델 등도 변환해 실행 가능.\n",
    "\n",
    "- 최적화 기능\n",
    "\n",
    "- 모델 경량화(Quantization: 정수/반정밀도 변환)\n",
    "\n",
    "  - 연산 최적화 및 하드웨어 가속기(TPU, GPU, NPU, DSP 등) 지원\n",
    "  - 광범위한 디바이스 지원\n",
    "    - Android / iOS\n",
    "    - Raspberry Pi, 마이크로컨트롤러(MCU) 같은 초저전력 디바이스까지 지원\n",
    "\n",
    "### **들어가며: AI Edge Torch**\n",
    "\n",
    "PyTorch 모델을 LiteRT(Lite Runtime, 구 TensorFlow Lite) 포맷으로 변환·최적화하기 위한 Python SDK 입니다.\n",
    "즉, PyTorch 개발자가 별도의 TensorFlow 변환 과정을 거치지 않고도 PyTorch 모델을 바로 LiteRT로 내보낼 수 있게 해주는 도구입니다.\n",
    "\n",
    "기존에는 PyTorch → ONNX → TFLite 변환 같은 복잡한 경로가 필요했는데, 이를 단순화 했습니다.\n",
    "\n",
    "\n",
    "### **실습 목차**\n",
    "\n",
    "이 노트북은 두 가지 주요 파트로 구성되어 있습니다.\n",
    "1. Sequence Classification 모델 변환\n",
    "2. 변환된 모델을 tflite interpreter 을 통해 실행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-mHPn3lO8Iv"
   },
   "source": [
    "# **실습1: Sequence Classification 모델 재구성 및 변환**\n",
    "\n",
    "해당 파트에서는 ai-edge-torch 라이브러리를 사용하여 사전 학습된 트랜스포머(Transformer) 모델을 시퀀스 분류(Sequence Classification) 작업에 맞게 수정한 뒤, TensorFlow Lite (TFLite) 형식으로 변환하는 전체 과정을 단계별로 알아봅니다.\n",
    "\n",
    "TFLite로 변환하면 모델을 모바일이나 엣지 디바이스에 배포하여 더 빠르고 효율적으로 추론을 수행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ubr_ONKgPYH7"
   },
   "source": [
    "먼저, 모델을 정의하고 변환하는 데 필요한 **라이브러리들을 임포트**합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50743,
     "status": "ok",
     "timestamp": 1756034767995,
     "user": {
      "displayName": "isu jeong",
      "userId": "03509833375949211012"
     },
     "user_tz": -540
    },
    "id": "Yb2QqxuRPbfM",
    "outputId": "1950fc03-97c6-48cd-91f5-fecec0fbb688"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:354: UserWarning: Device capability of jax unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# --- PyTorch 및 기본 라이브러리 ---\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional, Dict\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import safetensors.torch\n",
    "\n",
    "# --- transformer 라이브러리 ---\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "# --- ai_edge_torch 관련 라이브러리 ---\n",
    "# 모델 변환 및 양자화를 위한 핵심 도구\n",
    "import ai_edge_torch\n",
    "\n",
    "# 생성 모델 레이어 (Attention, TransformerBlock 등) 및 설정을 위한 유틸리티\n",
    "from ai_edge_torch.generative.layers import attention, builder\n",
    "import ai_edge_torch.generative.layers.attention_utils as attn_utils\n",
    "import ai_edge_torch.generative.layers.model_config as cfg\n",
    "\n",
    "# 사전 학습된 가중치를 불러오기 위한 로더 유틸리티\n",
    "import ai_edge_torch.generative.utilities.loader as loading_utils\n",
    "from safetensors import safe_open\n",
    "\n",
    "# quantization 을 위한 유틸리티\n",
    "from ai_edge_torch.generative.quantize import quant_recipes\n",
    "\n",
    "# --- google drive 연결 ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "root = '/content/drive/MyDrive/Upstage-AI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjNSme_QPlwP"
   },
   "source": [
    "다음으로 **시퀀스 분류 모델을 정의**합니다.\n",
    "\n",
    "### `ai_edge_torch`를 사용해 모델을 새로 정의하는 이유\n",
    "\n",
    "* **온디바이스(on-device) 환경**에 최적화된, 가볍고 효율적인 `TFLite` 모델을 생성하기 위함.\n",
    "* **HuggingFace:** 연구/개발 편의성 및 범용성 초점.\n",
    "* **`ai_edge_torch`:** 모바일 기기 등 제한된 환경에서의 **추론 성능 극대화** 초점.\n",
    "\n",
    "\n",
    "### 핵심 차이점\n",
    "\n",
    "#### 1. 타겟 환경의 차이: 서버 vs 엣지 디바이스\n",
    "\n",
    "* **HuggingFace `transformers`**\n",
    "    * **타겟:** 서버, PC 환경 (GPU).\n",
    "    * **우선순위:** 개발 편의성, 모델 유연성, 쉬운 학습.\n",
    "    * **특징:** `forward` 함수 내 다양한 분기문 및 동적 로직 포함 가능.\n",
    "* **`ai_edge_torch`**\n",
    "    * **타겟:** 모바일, IoT 등 엣지 디바이스.\n",
    "    * **고려사항:** 제한된 메모리, 연산 능력, 배터리.\n",
    "    * **요구사항:** 모든 구성 요소(레이어, 연산)가 `TFLite` 변환 및 모바일 하드웨어(NPU, DSP) 가속에 최적화되어야 함.\n",
    "\n",
    "#### 2. 모델 구조의 최적화 수준\n",
    "\n",
    "* **Hugging Face `transformers`**\n",
    "    * **구조:** `PyTorch` 표준 레이어(`nn.Linear`, `nn.LayerNorm` 등)와 파이썬 로직을 자유롭게 사용.\n",
    "    * **한계:** 일부 연산은 `TFLite` 변환 시 모바일 하드웨어에서 비효율적이거나 미지원.\n",
    "* **`ai_edge_torch`**\n",
    "    * **구조:** `TFLite` 변환 및 모바일 가속기에 최적화된 레이어만 사용하여 모델 \"재조립\".\n",
    "    * **특징:** 범용성보다 성능을 위해 의도적으로 모델 구조와 사용 가능 연산을 제한.\n",
    "\n",
    "#### 3. 정적 그래프(Static Graph)와 추적(Tracing)\n",
    "\n",
    "* **`TFLite` 변환 전제조건:** 모델의 연산 흐름이 **'정적 그래프(Static Graph)'** 형태로 고정되어야 함.\n",
    "* **Hugging Face `transformers`**\n",
    "    * `forward` 함수가 복잡하고 분기 처리가 많아 정적 그래프로 변환(추적)이 매우 어렵거나 불가능.\n",
    "* **`ai_edge_torch`**\n",
    "    * 모델 정의 단계부터 모든 레이어와 로직을 `TFLite` 변환기가 쉽게 추적하도록 간결하게 설계.\n",
    "    * 이것이 `ai_edge_torch` 부품으로 모델을 새로 만드는 ***가장 큰 이유*** 중 하나."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CgzhJvw3PdV"
   },
   "source": [
    "이전에 불러왔던 모델의 형태는 다음과 같습니다.\n",
    "SmolLM 모델로 `AutoModelForSequenceClassification` 을 생성하면 내부적으로 `LlamaForSequenceClassification` 을 호출하기 때문에 현재 이름이 `Llama` 으로 되어있습니다.\n",
    "\n",
    "```\n",
    "LlamaForSequenceClassification(\n",
    "  (model): LlamaModel(\n",
    "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
    "    (layers): ModuleList(\n",
    "      (0-29): 30 x LlamaDecoderLayer(\n",
    "        (self_attn): LlamaAttention(\n",
    "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
    "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
    "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
    "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
    "        )\n",
    "        (mlp): LlamaMLP(\n",
    "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
    "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
    "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
    "          (act_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
    "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
    "      )\n",
    "    )\n",
    "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
    "    (rotary_emb): LlamaRotaryEmbedding()\n",
    "  )\n",
    "  (score): Linear(in_features=576, out_features=2, bias=False)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds2ElCUl3S_1"
   },
   "source": [
    "다음으로 이에 맞춰 sequence classification model 을 정의해봅시다.\n",
    "\n",
    "이 함수에서는 SmolLM2이라는 특정 트랜스포머 모델의 아키텍처를 정의하고 sequence classification model 에 맞게 추가로 score layer 을 추가합니다. (앞 모듈에서 사용하였던 모델입니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQvJQgdiPLZ7"
   },
   "outputs": [],
   "source": [
    "class SequenceClassificationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    사전 학습된 트랜스포머 아키텍처를 기반으로 하는 텍스트 분류 모델입니다.\n",
    "    TFLite와 같은 엣지 디바이스 환경으로의 변환을 염두에 두고 설계되었습니다.\n",
    "    모델은 입력된 텍스트 시퀀스 전체의 의미를 압축하여 특정 클래스로 분류하는 작업을 수행합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: cfg.ModelConfig, num_classes: int = 2):\n",
    "        \"\"\"\n",
    "        모델의 레이어를 초기화합니다.\n",
    "\n",
    "        Args:\n",
    "            config (cfg.ModelConfig): 모델의 구조와 하이퍼파라미터를 담고 있는 설정 객체.\n",
    "            num_classes (int): 분류할 클래스의 개수.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # --- 모델의 주요 구성 요소 ---\n",
    "\n",
    "        # --- 1. 토큰 임베딩 레이어 ---\n",
    "        # 정수로 된 토큰 ID를 고차원의 벡터 표현으로 변환합니다.\n",
    "        # 이 벡터는 단어의 의미를 내포하게 됩니다.\n",
    "        self.tok_embedding = nn.Embedding(\n",
    "            config.vocab_size, config.embedding_dim, padding_idx=2,\n",
    "        )\n",
    "\n",
    "        # --- 2. 트랜스포머 블록 ---\n",
    "        # 모델의 핵심 엔진으로, 여러 개의 트랜스포머 블록이 쌓여 있습니다.\n",
    "        # 각 블록은 셀프 어텐션(self-attention) 메커니즘을 통해\n",
    "        # 문장 내 단어들의 문맥적 관계를 학습합니다.\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            attention.TransformerBlock(config.block_config(idx), config)\n",
    "            for idx in range(config.num_layers)\n",
    "        )\n",
    "\n",
    "        # --- 3. 최종 정규화 레이어 ---\n",
    "        # 모든 트랜스포머 블록을 통과한 출력은 불안정할 수 있으므로,\n",
    "        # 마지막으로 정규화를 적용하여 안정적인 값을 얻습니다.\n",
    "        self.final_norm = builder.build_norm(\n",
    "            config.embedding_dim, config.final_norm_config\n",
    "        )\n",
    "\n",
    "        # --- 4. 어텐션 마스크 캐시 ---\n",
    "        # 디코더-온리 모델의 특성상, 각 토큰이 자기 자신보다 뒤에 있는 토큰을\n",
    "        # 볼 수 없도록 가려주는 '인과 관계 마스크(causal mask)'가 필요합니다.\n",
    "        # 이 마스크를 미리 계산하여 캐싱해두면 추론 속도를 높일 수 있습니다.\n",
    "        self.mask_cache = attn_utils.build_causal_mask_cache(\n",
    "            size=config.kv_cache_max,\n",
    "        )\n",
    "\n",
    "        # --- 5. 분류 헤드 (Classification Head) ---\n",
    "        # 트랜스포머가 추출한 문장 전체의 의미 벡터를 입력받아,\n",
    "        # 최종적으로 각 클래스에 대한 점수(logit)를 출력하는 선형 레이어입니다.\n",
    "\n",
    "        # 문제 1: Classification 을 위해 score 레이어를 정의합니다.\n",
    "        # 이전 모듈에서 생성한 모델과 동일해야하므로 이름을 score 으로 지정합니다. bias 는 없습니다.\n",
    "        # [START CODE]\n",
    "        self.score = ??\n",
    "        # [END CODE]\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        # --- 0. 초기 설정 ---\n",
    "        # 입력 텐서로부터 배치 크기와 시퀀스 길이를 가져옵니다.\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        # --- 1. 임베딩 생성 ---\n",
    "        # 입력된 토큰 ID를 고차원의 벡터 표현(임베딩)으로 변환합니다.\n",
    "        input_embeds = self.tok_embedding(input_ids)\n",
    "\n",
    "        # --- 2. RoPE (회전 위치 임베딩) 준비 ---\n",
    "        # 모델 설정에서 어텐션 관련 설정을 가져옵니다.\n",
    "        attn_config = self.config.block_config(0).attn_config\n",
    "        # RoPE를 적용할 임베딩 차원의 크기를 계산합니다.\n",
    "        n_elem = int(attn_config.rotary_percentage * attn_config.head_dim)\n",
    "        # 시퀀스 길이에 맞는 위치 인덱스([0, 1, 2, ...])를 생성합니다.\n",
    "        input_pos = torch.arange(0, seq_len, device=input_ids.device)\n",
    "        # 위치 인덱스를 기반으로 RoPE 텐서를 생성합니다. 이는 모델이 토큰의 순서 정보를 학습하는 데 사용됩니다.\n",
    "        rope = self.config.build_rope(input_pos, n_elem, attn_config.rotary_base)\n",
    "\n",
    "        # --- 3. 어텐션 마스크 결합 ---\n",
    "        # 미리 계산된 인과 관계 마스크(causal mask) 캐시에서 현재 시퀀스 길이에 맞는 부분을 가져옵니다.\n",
    "        # 인과 관계 마스크는 각 토큰이 자기 자신과 이전 토큰에만 집중하도록 제한합니다.\n",
    "        causal_mask = self.mask_cache.index_select(2, input_pos)\n",
    "        causal_mask = causal_mask[:, :, :seq_len, :seq_len]\n",
    "\n",
    "        # attention_mask에서 패딩(padding) 부분(값이 0)을 찾아 boolean 마스크를 생성합니다.\n",
    "        padding_mask_bool = (attention_mask == 0)\n",
    "        # 패딩 마스크의 차원을 (batch, 1, 1, seq_len) 형태로 확장하여\n",
    "        # 어텐션 마스크 (batch, num_heads, seq_len, seq_len)와 연산이 가능하도록 만듭니다.\n",
    "        padding_mask_bool = padding_mask_bool.unsqueeze(1).unsqueeze(2)\n",
    "        # 인과 관계 마스크에 패딩 마스크를 결합합니다. 패딩 위치는 -무한대로 채워져 어텐션 계산에서 제외됩니다.\n",
    "        mask = causal_mask.masked_fill(padding_mask_bool, -torch.inf)\n",
    "\n",
    "        # --- 4. 트랜스포머 블록 통과 ---\n",
    "        x = input_embeds\n",
    "        # 설정에 따라 임베딩 벡터에 스케일링을 적용할 수 있습니다.\n",
    "        if self.config.embedding_scale is not None:\n",
    "            x = x * self.config.embedding_scale\n",
    "\n",
    "        # 준비된 입력(x), RoPE, 최종 마스크를 모든 트랜스포머 블록에 순차적으로 통과시킵니다.\n",
    "        for block in self.transformer_blocks:\n",
    "            # kv_cache 인자(None, None)는 텍스트 생성 시에만 사용되므로 여기서는 비워둡니다.\n",
    "            x = block(x, rope, mask, input_pos, None, None)\n",
    "\n",
    "        # 모든 블록을 통과한 최종 결과에 정규화(Normalization)를 적용합니다.\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # --- 5. 풀링(Pooling) 및 분류 ---\n",
    "        # attention_mask의 합계를 통해 각 시퀀스의 실제 길이를 계산합니다.\n",
    "        actual_seq_lengths = attention_mask.sum(dim=1)\n",
    "        # 실제 마지막 토큰의 인덱스를 계산합니다. (길이 - 1)\n",
    "        last_token_indices = actual_seq_lengths - 1\n",
    "        # 배치 내 각 항목을 선택하기 위한 인덱스를 생성합니다. (예: [0, 1, 2, ...])\n",
    "        batch_indices = torch.arange(batch_size, device=input_ids.device)\n",
    "        # x에서 각 시퀀스의 '실제 마지막 토큰'에 해당하는 은닉 상태 벡터만 추출합니다.\n",
    "        # 이 벡터가 문장 전체의 의미를 대표하는 벡터로 사용됩니다.\n",
    "        pooled_output = x[batch_indices, last_token_indices]\n",
    "\n",
    "        # 문제 2: 추출된 대표 벡터를 분류 레이어(score)에 통과시켜 최종 로짓(logits)을 계산합니다.\n",
    "        # [START CODE]\n",
    "        logits = ??\n",
    "        # [END CODE]\n",
    "\n",
    "        # --- 6. 결과 반환 ---\n",
    "        # 최종 로짓을 딕셔너리 형태로 반환합니다.\n",
    "        return {'logits': logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXfx0hpGRqaI"
   },
   "source": [
    "다음은 모델의 상세한 구조(레이어 수, 임베딩 차원 등)를 정의하는 `get_model_config` 함수를 작성합니다.\n",
    " 어텐션 헤드 개수, 레이어 수 등 모델의 모든 하이퍼파라미터가 여기에 명시됩니다. 해당 하이퍼파라미터들은 config.json 에서 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idIH8zwzPTDP"
   },
   "outputs": [],
   "source": [
    "def get_model_config() -> cfg.ModelConfig:\n",
    "    \"\"\"SmolLM 135M 모델의 설정을 반환합니다.\"\"\"\n",
    "\n",
    "    # 문제 3: 모델 생성을 위한 config 을 작성합니다.\n",
    "    # 모델이 저장된 경로의 config.json 을 참고하여 작성하세요.\n",
    "    # [START CODE]\n",
    "\n",
    "    attn_config = cfg.AttentionConfig(\n",
    "        num_heads=??,           # 어텐션 헤드의 수 (from config.json: num_attention_heads)\n",
    "        head_dim=??,            # 각 헤드의 차원 (from config.json: head_dim)\n",
    "        num_query_groups=??,    # Grouped Query Attention을 위한 그룹 수 (from config.json: num_key_value_heads)\n",
    "        rotary_base=??,         # from config.json: rope_theta\n",
    "        rotary_percentage=1.0,\n",
    "    )\n",
    "    ff_config = cfg.FeedForwardConfig(\n",
    "        type=cfg.FeedForwardType.GATED,\n",
    "        activation=cfg.ActivationConfig(cfg.ActivationType.SILU),\n",
    "        intermediate_size=??, # 피드 포워드 네트워크의 중간 차원 (from config.json: intermediate_size)\n",
    "    )\n",
    "    norm_config = cfg.NormalizationConfig(type=cfg.NormalizationType.RMS_NORM,\n",
    "                                          epsilon=1e-5)\n",
    "    block_config = cfg.TransformerBlockConfig(\n",
    "        attn_config=attn_config,\n",
    "        ff_config=ff_config,\n",
    "        pre_attention_norm_config=norm_config,\n",
    "        post_attention_norm_config=norm_config,\n",
    "    )\n",
    "    config = cfg.ModelConfig(\n",
    "        vocab_size=??,        # 어휘 사전의 크기 (from config.json: vocab_size)\n",
    "        num_layers=??,        # 트랜스포머 블록의 수 (from config.json: num_hidden_layers)\n",
    "        max_seq_len=??,       # 최대 시퀀스 길이\n",
    "        embedding_dim=??,     # 임베딩 벡터의 차원 (from config.json: hidden_size)\n",
    "        block_configs=block_config,\n",
    "        final_norm_config=norm_config,\n",
    "    )\n",
    "    # [END CODE]\n",
    "\n",
    "    config.block_config(0).attn_config.rotary_base = 100000\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_6MDQ7wSrwZ"
   },
   "source": [
    "모델을 만들고 만든 모델을 반환해주는 `build_model` 함수를 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JH1WKcXGLSzL"
   },
   "outputs": [],
   "source": [
    "# build_sequence_classification_model을 더 간단히 호출하기 위한 래퍼(wrapper) 함수\n",
    "def build_model(\n",
    "        config: cfg.ModelConfig,\n",
    "        model_class: type[nn.Module] = SequenceClassificationModel,\n",
    "        num_classes: int = 2,\n",
    "    ) -> nn.Module:\n",
    "    transformer = model_class(config, num_classes=num_classes)\n",
    "    transformer.eval()\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2648,
     "status": "ok",
     "timestamp": 1756038223582,
     "user": {
      "displayName": "isu jeong",
      "userId": "03509833375949211012"
     },
     "user_tz": -540
    },
    "id": "wxLBmJVYhSJu",
    "outputId": "c4ac5bf2-f8d8-4f6b-a3b1-f4be857182cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassificationModel(\n",
       "  (tok_embedding): Embedding(49152, 576, padding_idx=2)\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-29): 30 x TransformerBlock(\n",
       "      (pre_atten_norm): RMSNorm()\n",
       "      (atten_func): CausalSelfAttention(\n",
       "        (qkv_projection): Linear(in_features=576, out_features=960, bias=False)\n",
       "        (output_projection): Linear(in_features=576, out_features=576, bias=False)\n",
       "      )\n",
       "      (post_atten_norm): RMSNorm()\n",
       "      (ff): GatedFeedForward(\n",
       "        (w1): Linear(in_features=576, out_features=1536, bias=False)\n",
       "        (w2): Linear(in_features=1536, out_features=576, bias=False)\n",
       "        (w3): Linear(in_features=576, out_features=1536, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (score): Linear(in_features=576, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문제 4: 작성한 함수를 호출하여 모델을 생성합니다. 생성한 모델을 출력하여 구조를 확인합니다.\n",
    "# [START CODE]\n",
    "# config 을 만듭니다.\n",
    "config = ??\n",
    "# model 을 만듭니다.\n",
    "model = ??\n",
    "# model 을 출력합니다.\n",
    "model\n",
    "# [END CODE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xTCyQqLbgV4"
   },
   "source": [
    "다음으로 만들어낸 Pytorch 모델의 가중치를 이전에 미세조정한 모델의 가중치로 덮어씌웁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rZ15QKxbqWC"
   },
   "source": [
    "먼저 이전에 미세조정한 모델의 가중치를 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1756038223713,
     "user": {
      "displayName": "isu jeong",
      "userId": "03509833375949211012"
     },
     "user_tz": -540
    },
    "id": "GEYxpYmtYDZC",
    "outputId": "97732681-7c89-44e1-b9f6-1f4b00e3c0ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.embed_tokens.weight',\n",
       " 'model.layers.0.input_layernorm.weight',\n",
       " 'model.layers.0.mlp.down_proj.weight',\n",
       " 'model.layers.0.mlp.gate_proj.weight',\n",
       " 'model.layers.0.mlp.up_proj.weight',\n",
       " 'model.layers.0.post_attention_layernorm.weight',\n",
       " 'model.layers.0.self_attn.k_proj.weight',\n",
       " 'model.layers.0.self_attn.o_proj.weight',\n",
       " 'model.layers.0.self_attn.q_proj.weight',\n",
       " 'model.layers.0.self_attn.v_proj.weight',\n",
       " 'model.layers.1.input_layernorm.weight',\n",
       " 'model.layers.1.mlp.down_proj.weight',\n",
       " 'model.layers.1.mlp.gate_proj.weight',\n",
       " 'model.layers.1.mlp.up_proj.weight',\n",
       " 'model.layers.1.post_attention_layernorm.weight',\n",
       " 'model.layers.1.self_attn.k_proj.weight',\n",
       " 'model.layers.1.self_attn.o_proj.weight',\n",
       " 'model.layers.1.self_attn.q_proj.weight',\n",
       " 'model.layers.1.self_attn.v_proj.weight',\n",
       " 'model.layers.10.input_layernorm.weight',\n",
       " 'model.layers.10.mlp.down_proj.weight',\n",
       " 'model.layers.10.mlp.gate_proj.weight',\n",
       " 'model.layers.10.mlp.up_proj.weight',\n",
       " 'model.layers.10.post_attention_layernorm.weight',\n",
       " 'model.layers.10.self_attn.k_proj.weight',\n",
       " 'model.layers.10.self_attn.o_proj.weight',\n",
       " 'model.layers.10.self_attn.q_proj.weight',\n",
       " 'model.layers.10.self_attn.v_proj.weight',\n",
       " 'model.layers.11.input_layernorm.weight',\n",
       " 'model.layers.11.mlp.down_proj.weight',\n",
       " 'model.layers.11.mlp.gate_proj.weight',\n",
       " 'model.layers.11.mlp.up_proj.weight',\n",
       " 'model.layers.11.post_attention_layernorm.weight',\n",
       " 'model.layers.11.self_attn.k_proj.weight',\n",
       " 'model.layers.11.self_attn.o_proj.weight',\n",
       " 'model.layers.11.self_attn.q_proj.weight',\n",
       " 'model.layers.11.self_attn.v_proj.weight',\n",
       " 'model.layers.12.input_layernorm.weight',\n",
       " 'model.layers.12.mlp.down_proj.weight',\n",
       " 'model.layers.12.mlp.gate_proj.weight',\n",
       " 'model.layers.12.mlp.up_proj.weight',\n",
       " 'model.layers.12.post_attention_layernorm.weight',\n",
       " 'model.layers.12.self_attn.k_proj.weight',\n",
       " 'model.layers.12.self_attn.o_proj.weight',\n",
       " 'model.layers.12.self_attn.q_proj.weight',\n",
       " 'model.layers.12.self_attn.v_proj.weight',\n",
       " 'model.layers.13.input_layernorm.weight',\n",
       " 'model.layers.13.mlp.down_proj.weight',\n",
       " 'model.layers.13.mlp.gate_proj.weight',\n",
       " 'model.layers.13.mlp.up_proj.weight',\n",
       " 'model.layers.13.post_attention_layernorm.weight',\n",
       " 'model.layers.13.self_attn.k_proj.weight',\n",
       " 'model.layers.13.self_attn.o_proj.weight',\n",
       " 'model.layers.13.self_attn.q_proj.weight',\n",
       " 'model.layers.13.self_attn.v_proj.weight',\n",
       " 'model.layers.14.input_layernorm.weight',\n",
       " 'model.layers.14.mlp.down_proj.weight',\n",
       " 'model.layers.14.mlp.gate_proj.weight',\n",
       " 'model.layers.14.mlp.up_proj.weight',\n",
       " 'model.layers.14.post_attention_layernorm.weight',\n",
       " 'model.layers.14.self_attn.k_proj.weight',\n",
       " 'model.layers.14.self_attn.o_proj.weight',\n",
       " 'model.layers.14.self_attn.q_proj.weight',\n",
       " 'model.layers.14.self_attn.v_proj.weight',\n",
       " 'model.layers.15.input_layernorm.weight',\n",
       " 'model.layers.15.mlp.down_proj.weight',\n",
       " 'model.layers.15.mlp.gate_proj.weight',\n",
       " 'model.layers.15.mlp.up_proj.weight',\n",
       " 'model.layers.15.post_attention_layernorm.weight',\n",
       " 'model.layers.15.self_attn.k_proj.weight',\n",
       " 'model.layers.15.self_attn.o_proj.weight',\n",
       " 'model.layers.15.self_attn.q_proj.weight',\n",
       " 'model.layers.15.self_attn.v_proj.weight',\n",
       " 'model.layers.16.input_layernorm.weight',\n",
       " 'model.layers.16.mlp.down_proj.weight',\n",
       " 'model.layers.16.mlp.gate_proj.weight',\n",
       " 'model.layers.16.mlp.up_proj.weight',\n",
       " 'model.layers.16.post_attention_layernorm.weight',\n",
       " 'model.layers.16.self_attn.k_proj.weight',\n",
       " 'model.layers.16.self_attn.o_proj.weight',\n",
       " 'model.layers.16.self_attn.q_proj.weight',\n",
       " 'model.layers.16.self_attn.v_proj.weight',\n",
       " 'model.layers.17.input_layernorm.weight',\n",
       " 'model.layers.17.mlp.down_proj.weight',\n",
       " 'model.layers.17.mlp.gate_proj.weight',\n",
       " 'model.layers.17.mlp.up_proj.weight',\n",
       " 'model.layers.17.post_attention_layernorm.weight',\n",
       " 'model.layers.17.self_attn.k_proj.weight',\n",
       " 'model.layers.17.self_attn.o_proj.weight',\n",
       " 'model.layers.17.self_attn.q_proj.weight',\n",
       " 'model.layers.17.self_attn.v_proj.weight',\n",
       " 'model.layers.18.input_layernorm.weight',\n",
       " 'model.layers.18.mlp.down_proj.weight',\n",
       " 'model.layers.18.mlp.gate_proj.weight',\n",
       " 'model.layers.18.mlp.up_proj.weight',\n",
       " 'model.layers.18.post_attention_layernorm.weight',\n",
       " 'model.layers.18.self_attn.k_proj.weight',\n",
       " 'model.layers.18.self_attn.o_proj.weight',\n",
       " 'model.layers.18.self_attn.q_proj.weight',\n",
       " 'model.layers.18.self_attn.v_proj.weight',\n",
       " 'model.layers.19.input_layernorm.weight',\n",
       " 'model.layers.19.mlp.down_proj.weight',\n",
       " 'model.layers.19.mlp.gate_proj.weight',\n",
       " 'model.layers.19.mlp.up_proj.weight',\n",
       " 'model.layers.19.post_attention_layernorm.weight',\n",
       " 'model.layers.19.self_attn.k_proj.weight',\n",
       " 'model.layers.19.self_attn.o_proj.weight',\n",
       " 'model.layers.19.self_attn.q_proj.weight',\n",
       " 'model.layers.19.self_attn.v_proj.weight',\n",
       " 'model.layers.2.input_layernorm.weight',\n",
       " 'model.layers.2.mlp.down_proj.weight',\n",
       " 'model.layers.2.mlp.gate_proj.weight',\n",
       " 'model.layers.2.mlp.up_proj.weight',\n",
       " 'model.layers.2.post_attention_layernorm.weight',\n",
       " 'model.layers.2.self_attn.k_proj.weight',\n",
       " 'model.layers.2.self_attn.o_proj.weight',\n",
       " 'model.layers.2.self_attn.q_proj.weight',\n",
       " 'model.layers.2.self_attn.v_proj.weight',\n",
       " 'model.layers.20.input_layernorm.weight',\n",
       " 'model.layers.20.mlp.down_proj.weight',\n",
       " 'model.layers.20.mlp.gate_proj.weight',\n",
       " 'model.layers.20.mlp.up_proj.weight',\n",
       " 'model.layers.20.post_attention_layernorm.weight',\n",
       " 'model.layers.20.self_attn.k_proj.weight',\n",
       " 'model.layers.20.self_attn.o_proj.weight',\n",
       " 'model.layers.20.self_attn.q_proj.weight',\n",
       " 'model.layers.20.self_attn.v_proj.weight',\n",
       " 'model.layers.21.input_layernorm.weight',\n",
       " 'model.layers.21.mlp.down_proj.weight',\n",
       " 'model.layers.21.mlp.gate_proj.weight',\n",
       " 'model.layers.21.mlp.up_proj.weight',\n",
       " 'model.layers.21.post_attention_layernorm.weight',\n",
       " 'model.layers.21.self_attn.k_proj.weight',\n",
       " 'model.layers.21.self_attn.o_proj.weight',\n",
       " 'model.layers.21.self_attn.q_proj.weight',\n",
       " 'model.layers.21.self_attn.v_proj.weight',\n",
       " 'model.layers.22.input_layernorm.weight',\n",
       " 'model.layers.22.mlp.down_proj.weight',\n",
       " 'model.layers.22.mlp.gate_proj.weight',\n",
       " 'model.layers.22.mlp.up_proj.weight',\n",
       " 'model.layers.22.post_attention_layernorm.weight',\n",
       " 'model.layers.22.self_attn.k_proj.weight',\n",
       " 'model.layers.22.self_attn.o_proj.weight',\n",
       " 'model.layers.22.self_attn.q_proj.weight',\n",
       " 'model.layers.22.self_attn.v_proj.weight',\n",
       " 'model.layers.23.input_layernorm.weight',\n",
       " 'model.layers.23.mlp.down_proj.weight',\n",
       " 'model.layers.23.mlp.gate_proj.weight',\n",
       " 'model.layers.23.mlp.up_proj.weight',\n",
       " 'model.layers.23.post_attention_layernorm.weight',\n",
       " 'model.layers.23.self_attn.k_proj.weight',\n",
       " 'model.layers.23.self_attn.o_proj.weight',\n",
       " 'model.layers.23.self_attn.q_proj.weight',\n",
       " 'model.layers.23.self_attn.v_proj.weight',\n",
       " 'model.layers.24.input_layernorm.weight',\n",
       " 'model.layers.24.mlp.down_proj.weight',\n",
       " 'model.layers.24.mlp.gate_proj.weight',\n",
       " 'model.layers.24.mlp.up_proj.weight',\n",
       " 'model.layers.24.post_attention_layernorm.weight',\n",
       " 'model.layers.24.self_attn.k_proj.weight',\n",
       " 'model.layers.24.self_attn.o_proj.weight',\n",
       " 'model.layers.24.self_attn.q_proj.weight',\n",
       " 'model.layers.24.self_attn.v_proj.weight',\n",
       " 'model.layers.25.input_layernorm.weight',\n",
       " 'model.layers.25.mlp.down_proj.weight',\n",
       " 'model.layers.25.mlp.gate_proj.weight',\n",
       " 'model.layers.25.mlp.up_proj.weight',\n",
       " 'model.layers.25.post_attention_layernorm.weight',\n",
       " 'model.layers.25.self_attn.k_proj.weight',\n",
       " 'model.layers.25.self_attn.o_proj.weight',\n",
       " 'model.layers.25.self_attn.q_proj.weight',\n",
       " 'model.layers.25.self_attn.v_proj.weight',\n",
       " 'model.layers.26.input_layernorm.weight',\n",
       " 'model.layers.26.mlp.down_proj.weight',\n",
       " 'model.layers.26.mlp.gate_proj.weight',\n",
       " 'model.layers.26.mlp.up_proj.weight',\n",
       " 'model.layers.26.post_attention_layernorm.weight',\n",
       " 'model.layers.26.self_attn.k_proj.weight',\n",
       " 'model.layers.26.self_attn.o_proj.weight',\n",
       " 'model.layers.26.self_attn.q_proj.weight',\n",
       " 'model.layers.26.self_attn.v_proj.weight',\n",
       " 'model.layers.27.input_layernorm.weight',\n",
       " 'model.layers.27.mlp.down_proj.weight',\n",
       " 'model.layers.27.mlp.gate_proj.weight',\n",
       " 'model.layers.27.mlp.up_proj.weight',\n",
       " 'model.layers.27.post_attention_layernorm.weight',\n",
       " 'model.layers.27.self_attn.k_proj.weight',\n",
       " 'model.layers.27.self_attn.o_proj.weight',\n",
       " 'model.layers.27.self_attn.q_proj.weight',\n",
       " 'model.layers.27.self_attn.v_proj.weight',\n",
       " 'model.layers.28.input_layernorm.weight',\n",
       " 'model.layers.28.mlp.down_proj.weight',\n",
       " 'model.layers.28.mlp.gate_proj.weight',\n",
       " 'model.layers.28.mlp.up_proj.weight',\n",
       " 'model.layers.28.post_attention_layernorm.weight',\n",
       " 'model.layers.28.self_attn.k_proj.weight',\n",
       " 'model.layers.28.self_attn.o_proj.weight',\n",
       " 'model.layers.28.self_attn.q_proj.weight',\n",
       " 'model.layers.28.self_attn.v_proj.weight',\n",
       " 'model.layers.29.input_layernorm.weight',\n",
       " 'model.layers.29.mlp.down_proj.weight',\n",
       " 'model.layers.29.mlp.gate_proj.weight',\n",
       " 'model.layers.29.mlp.up_proj.weight',\n",
       " 'model.layers.29.post_attention_layernorm.weight',\n",
       " 'model.layers.29.self_attn.k_proj.weight',\n",
       " 'model.layers.29.self_attn.o_proj.weight',\n",
       " 'model.layers.29.self_attn.q_proj.weight',\n",
       " 'model.layers.29.self_attn.v_proj.weight',\n",
       " 'model.layers.3.input_layernorm.weight',\n",
       " 'model.layers.3.mlp.down_proj.weight',\n",
       " 'model.layers.3.mlp.gate_proj.weight',\n",
       " 'model.layers.3.mlp.up_proj.weight',\n",
       " 'model.layers.3.post_attention_layernorm.weight',\n",
       " 'model.layers.3.self_attn.k_proj.weight',\n",
       " 'model.layers.3.self_attn.o_proj.weight',\n",
       " 'model.layers.3.self_attn.q_proj.weight',\n",
       " 'model.layers.3.self_attn.v_proj.weight',\n",
       " 'model.layers.4.input_layernorm.weight',\n",
       " 'model.layers.4.mlp.down_proj.weight',\n",
       " 'model.layers.4.mlp.gate_proj.weight',\n",
       " 'model.layers.4.mlp.up_proj.weight',\n",
       " 'model.layers.4.post_attention_layernorm.weight',\n",
       " 'model.layers.4.self_attn.k_proj.weight',\n",
       " 'model.layers.4.self_attn.o_proj.weight',\n",
       " 'model.layers.4.self_attn.q_proj.weight',\n",
       " 'model.layers.4.self_attn.v_proj.weight',\n",
       " 'model.layers.5.input_layernorm.weight',\n",
       " 'model.layers.5.mlp.down_proj.weight',\n",
       " 'model.layers.5.mlp.gate_proj.weight',\n",
       " 'model.layers.5.mlp.up_proj.weight',\n",
       " 'model.layers.5.post_attention_layernorm.weight',\n",
       " 'model.layers.5.self_attn.k_proj.weight',\n",
       " 'model.layers.5.self_attn.o_proj.weight',\n",
       " 'model.layers.5.self_attn.q_proj.weight',\n",
       " 'model.layers.5.self_attn.v_proj.weight',\n",
       " 'model.layers.6.input_layernorm.weight',\n",
       " 'model.layers.6.mlp.down_proj.weight',\n",
       " 'model.layers.6.mlp.gate_proj.weight',\n",
       " 'model.layers.6.mlp.up_proj.weight',\n",
       " 'model.layers.6.post_attention_layernorm.weight',\n",
       " 'model.layers.6.self_attn.k_proj.weight',\n",
       " 'model.layers.6.self_attn.o_proj.weight',\n",
       " 'model.layers.6.self_attn.q_proj.weight',\n",
       " 'model.layers.6.self_attn.v_proj.weight',\n",
       " 'model.layers.7.input_layernorm.weight',\n",
       " 'model.layers.7.mlp.down_proj.weight',\n",
       " 'model.layers.7.mlp.gate_proj.weight',\n",
       " 'model.layers.7.mlp.up_proj.weight',\n",
       " 'model.layers.7.post_attention_layernorm.weight',\n",
       " 'model.layers.7.self_attn.k_proj.weight',\n",
       " 'model.layers.7.self_attn.o_proj.weight',\n",
       " 'model.layers.7.self_attn.q_proj.weight',\n",
       " 'model.layers.7.self_attn.v_proj.weight',\n",
       " 'model.layers.8.input_layernorm.weight',\n",
       " 'model.layers.8.mlp.down_proj.weight',\n",
       " 'model.layers.8.mlp.gate_proj.weight',\n",
       " 'model.layers.8.mlp.up_proj.weight',\n",
       " 'model.layers.8.post_attention_layernorm.weight',\n",
       " 'model.layers.8.self_attn.k_proj.weight',\n",
       " 'model.layers.8.self_attn.o_proj.weight',\n",
       " 'model.layers.8.self_attn.q_proj.weight',\n",
       " 'model.layers.8.self_attn.v_proj.weight',\n",
       " 'model.layers.9.input_layernorm.weight',\n",
       " 'model.layers.9.mlp.down_proj.weight',\n",
       " 'model.layers.9.mlp.gate_proj.weight',\n",
       " 'model.layers.9.mlp.up_proj.weight',\n",
       " 'model.layers.9.post_attention_layernorm.weight',\n",
       " 'model.layers.9.self_attn.k_proj.weight',\n",
       " 'model.layers.9.self_attn.o_proj.weight',\n",
       " 'model.layers.9.self_attn.q_proj.weight',\n",
       " 'model.layers.9.self_attn.v_proj.weight',\n",
       " 'model.norm.weight',\n",
       " 'score.weight']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 자신의 경로에 맞게 경로를 수정합니다.\n",
    "# 저장된 모델을 safetensor 형태로 불러와 weight 의 key 을 출력합니다.\n",
    "smollm_checkpoint_path = f'{root}/module-14/smollm2_merged_for_inference'\n",
    "tensors = safe_open(f'{smollm_checkpoint_path}/model.safetensors', framework=\"pt\")\n",
    "\n",
    "tensors.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRMhhppzcwjs"
   },
   "source": [
    "다음으로 생성한 모델의 가중치를 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1756038225131,
     "user": {
      "displayName": "isu jeong",
      "userId": "03509833375949211012"
     },
     "user_tz": -540
    },
    "id": "KJNy0fkqY2KZ",
    "outputId": "a1620b44-66cb-485d-d7fa-2d9a27c861a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['tok_embedding.weight', 'transformer_blocks.0.pre_atten_norm.weight', 'transformer_blocks.0.atten_func.qkv_projection.weight', 'transformer_blocks.0.atten_func.output_projection.weight', 'transformer_blocks.0.post_atten_norm.weight', 'transformer_blocks.0.ff.w1.weight', 'transformer_blocks.0.ff.w2.weight', 'transformer_blocks.0.ff.w3.weight', 'transformer_blocks.1.pre_atten_norm.weight', 'transformer_blocks.1.atten_func.qkv_projection.weight', 'transformer_blocks.1.atten_func.output_projection.weight', 'transformer_blocks.1.post_atten_norm.weight', 'transformer_blocks.1.ff.w1.weight', 'transformer_blocks.1.ff.w2.weight', 'transformer_blocks.1.ff.w3.weight', 'transformer_blocks.2.pre_atten_norm.weight', 'transformer_blocks.2.atten_func.qkv_projection.weight', 'transformer_blocks.2.atten_func.output_projection.weight', 'transformer_blocks.2.post_atten_norm.weight', 'transformer_blocks.2.ff.w1.weight', 'transformer_blocks.2.ff.w2.weight', 'transformer_blocks.2.ff.w3.weight', 'transformer_blocks.3.pre_atten_norm.weight', 'transformer_blocks.3.atten_func.qkv_projection.weight', 'transformer_blocks.3.atten_func.output_projection.weight', 'transformer_blocks.3.post_atten_norm.weight', 'transformer_blocks.3.ff.w1.weight', 'transformer_blocks.3.ff.w2.weight', 'transformer_blocks.3.ff.w3.weight', 'transformer_blocks.4.pre_atten_norm.weight', 'transformer_blocks.4.atten_func.qkv_projection.weight', 'transformer_blocks.4.atten_func.output_projection.weight', 'transformer_blocks.4.post_atten_norm.weight', 'transformer_blocks.4.ff.w1.weight', 'transformer_blocks.4.ff.w2.weight', 'transformer_blocks.4.ff.w3.weight', 'transformer_blocks.5.pre_atten_norm.weight', 'transformer_blocks.5.atten_func.qkv_projection.weight', 'transformer_blocks.5.atten_func.output_projection.weight', 'transformer_blocks.5.post_atten_norm.weight', 'transformer_blocks.5.ff.w1.weight', 'transformer_blocks.5.ff.w2.weight', 'transformer_blocks.5.ff.w3.weight', 'transformer_blocks.6.pre_atten_norm.weight', 'transformer_blocks.6.atten_func.qkv_projection.weight', 'transformer_blocks.6.atten_func.output_projection.weight', 'transformer_blocks.6.post_atten_norm.weight', 'transformer_blocks.6.ff.w1.weight', 'transformer_blocks.6.ff.w2.weight', 'transformer_blocks.6.ff.w3.weight', 'transformer_blocks.7.pre_atten_norm.weight', 'transformer_blocks.7.atten_func.qkv_projection.weight', 'transformer_blocks.7.atten_func.output_projection.weight', 'transformer_blocks.7.post_atten_norm.weight', 'transformer_blocks.7.ff.w1.weight', 'transformer_blocks.7.ff.w2.weight', 'transformer_blocks.7.ff.w3.weight', 'transformer_blocks.8.pre_atten_norm.weight', 'transformer_blocks.8.atten_func.qkv_projection.weight', 'transformer_blocks.8.atten_func.output_projection.weight', 'transformer_blocks.8.post_atten_norm.weight', 'transformer_blocks.8.ff.w1.weight', 'transformer_blocks.8.ff.w2.weight', 'transformer_blocks.8.ff.w3.weight', 'transformer_blocks.9.pre_atten_norm.weight', 'transformer_blocks.9.atten_func.qkv_projection.weight', 'transformer_blocks.9.atten_func.output_projection.weight', 'transformer_blocks.9.post_atten_norm.weight', 'transformer_blocks.9.ff.w1.weight', 'transformer_blocks.9.ff.w2.weight', 'transformer_blocks.9.ff.w3.weight', 'transformer_blocks.10.pre_atten_norm.weight', 'transformer_blocks.10.atten_func.qkv_projection.weight', 'transformer_blocks.10.atten_func.output_projection.weight', 'transformer_blocks.10.post_atten_norm.weight', 'transformer_blocks.10.ff.w1.weight', 'transformer_blocks.10.ff.w2.weight', 'transformer_blocks.10.ff.w3.weight', 'transformer_blocks.11.pre_atten_norm.weight', 'transformer_blocks.11.atten_func.qkv_projection.weight', 'transformer_blocks.11.atten_func.output_projection.weight', 'transformer_blocks.11.post_atten_norm.weight', 'transformer_blocks.11.ff.w1.weight', 'transformer_blocks.11.ff.w2.weight', 'transformer_blocks.11.ff.w3.weight', 'transformer_blocks.12.pre_atten_norm.weight', 'transformer_blocks.12.atten_func.qkv_projection.weight', 'transformer_blocks.12.atten_func.output_projection.weight', 'transformer_blocks.12.post_atten_norm.weight', 'transformer_blocks.12.ff.w1.weight', 'transformer_blocks.12.ff.w2.weight', 'transformer_blocks.12.ff.w3.weight', 'transformer_blocks.13.pre_atten_norm.weight', 'transformer_blocks.13.atten_func.qkv_projection.weight', 'transformer_blocks.13.atten_func.output_projection.weight', 'transformer_blocks.13.post_atten_norm.weight', 'transformer_blocks.13.ff.w1.weight', 'transformer_blocks.13.ff.w2.weight', 'transformer_blocks.13.ff.w3.weight', 'transformer_blocks.14.pre_atten_norm.weight', 'transformer_blocks.14.atten_func.qkv_projection.weight', 'transformer_blocks.14.atten_func.output_projection.weight', 'transformer_blocks.14.post_atten_norm.weight', 'transformer_blocks.14.ff.w1.weight', 'transformer_blocks.14.ff.w2.weight', 'transformer_blocks.14.ff.w3.weight', 'transformer_blocks.15.pre_atten_norm.weight', 'transformer_blocks.15.atten_func.qkv_projection.weight', 'transformer_blocks.15.atten_func.output_projection.weight', 'transformer_blocks.15.post_atten_norm.weight', 'transformer_blocks.15.ff.w1.weight', 'transformer_blocks.15.ff.w2.weight', 'transformer_blocks.15.ff.w3.weight', 'transformer_blocks.16.pre_atten_norm.weight', 'transformer_blocks.16.atten_func.qkv_projection.weight', 'transformer_blocks.16.atten_func.output_projection.weight', 'transformer_blocks.16.post_atten_norm.weight', 'transformer_blocks.16.ff.w1.weight', 'transformer_blocks.16.ff.w2.weight', 'transformer_blocks.16.ff.w3.weight', 'transformer_blocks.17.pre_atten_norm.weight', 'transformer_blocks.17.atten_func.qkv_projection.weight', 'transformer_blocks.17.atten_func.output_projection.weight', 'transformer_blocks.17.post_atten_norm.weight', 'transformer_blocks.17.ff.w1.weight', 'transformer_blocks.17.ff.w2.weight', 'transformer_blocks.17.ff.w3.weight', 'transformer_blocks.18.pre_atten_norm.weight', 'transformer_blocks.18.atten_func.qkv_projection.weight', 'transformer_blocks.18.atten_func.output_projection.weight', 'transformer_blocks.18.post_atten_norm.weight', 'transformer_blocks.18.ff.w1.weight', 'transformer_blocks.18.ff.w2.weight', 'transformer_blocks.18.ff.w3.weight', 'transformer_blocks.19.pre_atten_norm.weight', 'transformer_blocks.19.atten_func.qkv_projection.weight', 'transformer_blocks.19.atten_func.output_projection.weight', 'transformer_blocks.19.post_atten_norm.weight', 'transformer_blocks.19.ff.w1.weight', 'transformer_blocks.19.ff.w2.weight', 'transformer_blocks.19.ff.w3.weight', 'transformer_blocks.20.pre_atten_norm.weight', 'transformer_blocks.20.atten_func.qkv_projection.weight', 'transformer_blocks.20.atten_func.output_projection.weight', 'transformer_blocks.20.post_atten_norm.weight', 'transformer_blocks.20.ff.w1.weight', 'transformer_blocks.20.ff.w2.weight', 'transformer_blocks.20.ff.w3.weight', 'transformer_blocks.21.pre_atten_norm.weight', 'transformer_blocks.21.atten_func.qkv_projection.weight', 'transformer_blocks.21.atten_func.output_projection.weight', 'transformer_blocks.21.post_atten_norm.weight', 'transformer_blocks.21.ff.w1.weight', 'transformer_blocks.21.ff.w2.weight', 'transformer_blocks.21.ff.w3.weight', 'transformer_blocks.22.pre_atten_norm.weight', 'transformer_blocks.22.atten_func.qkv_projection.weight', 'transformer_blocks.22.atten_func.output_projection.weight', 'transformer_blocks.22.post_atten_norm.weight', 'transformer_blocks.22.ff.w1.weight', 'transformer_blocks.22.ff.w2.weight', 'transformer_blocks.22.ff.w3.weight', 'transformer_blocks.23.pre_atten_norm.weight', 'transformer_blocks.23.atten_func.qkv_projection.weight', 'transformer_blocks.23.atten_func.output_projection.weight', 'transformer_blocks.23.post_atten_norm.weight', 'transformer_blocks.23.ff.w1.weight', 'transformer_blocks.23.ff.w2.weight', 'transformer_blocks.23.ff.w3.weight', 'transformer_blocks.24.pre_atten_norm.weight', 'transformer_blocks.24.atten_func.qkv_projection.weight', 'transformer_blocks.24.atten_func.output_projection.weight', 'transformer_blocks.24.post_atten_norm.weight', 'transformer_blocks.24.ff.w1.weight', 'transformer_blocks.24.ff.w2.weight', 'transformer_blocks.24.ff.w3.weight', 'transformer_blocks.25.pre_atten_norm.weight', 'transformer_blocks.25.atten_func.qkv_projection.weight', 'transformer_blocks.25.atten_func.output_projection.weight', 'transformer_blocks.25.post_atten_norm.weight', 'transformer_blocks.25.ff.w1.weight', 'transformer_blocks.25.ff.w2.weight', 'transformer_blocks.25.ff.w3.weight', 'transformer_blocks.26.pre_atten_norm.weight', 'transformer_blocks.26.atten_func.qkv_projection.weight', 'transformer_blocks.26.atten_func.output_projection.weight', 'transformer_blocks.26.post_atten_norm.weight', 'transformer_blocks.26.ff.w1.weight', 'transformer_blocks.26.ff.w2.weight', 'transformer_blocks.26.ff.w3.weight', 'transformer_blocks.27.pre_atten_norm.weight', 'transformer_blocks.27.atten_func.qkv_projection.weight', 'transformer_blocks.27.atten_func.output_projection.weight', 'transformer_blocks.27.post_atten_norm.weight', 'transformer_blocks.27.ff.w1.weight', 'transformer_blocks.27.ff.w2.weight', 'transformer_blocks.27.ff.w3.weight', 'transformer_blocks.28.pre_atten_norm.weight', 'transformer_blocks.28.atten_func.qkv_projection.weight', 'transformer_blocks.28.atten_func.output_projection.weight', 'transformer_blocks.28.post_atten_norm.weight', 'transformer_blocks.28.ff.w1.weight', 'transformer_blocks.28.ff.w2.weight', 'transformer_blocks.28.ff.w3.weight', 'transformer_blocks.29.pre_atten_norm.weight', 'transformer_blocks.29.atten_func.qkv_projection.weight', 'transformer_blocks.29.atten_func.output_projection.weight', 'transformer_blocks.29.post_atten_norm.weight', 'transformer_blocks.29.ff.w1.weight', 'transformer_blocks.29.ff.w2.weight', 'transformer_blocks.29.ff.w3.weight', 'final_norm.weight', 'score.weight'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_KwNHa-c5MY"
   },
   "source": [
    "두 개의 가중치, 정확히는 가중치의 key 을 비교했을 때, key 가 다른것을 확인할 수 있습니다.\n",
    "이렇게 key 가 다르면 가중치를 불러올 수 없기 때문에 이를 고려해서 가중치를 로드해줘야 합니다.\n",
    "\n",
    "`*.safetensor` 형태의 저장된 모델 가중치로부터 가중치를 불러와 모델의 가중치에 매핑하는 함수를 구현합니다.\n",
    "\n",
    "\n",
    "> - (참고) fused qkv kernel 에 대한 이해를 위해 아래의 글을 참고하시면 좋습니다.\n",
    "    - [transformer](https://www.stephendiehl.com/posts/mlir_transformers/): transformer 구조에 대해 설명합니다. attnetion 에 대한 그림을 보았을 때, qkv fused kernel 을 사용하면 데이터 I/O 을 줄일 수 있을 것 입니다. \n",
    "    - [grouped query attention](https://www.geeksforgeeks.org/deep-learning/grouped-query-attention-gqa/) : grouped query attention (GQA) 에 대한 설명을 담고 있습니다. 왜 q, k, v projection 에 대한 weight 을 다르게 합치는지를 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwy2--fmY2IR"
   },
   "outputs": [],
   "source": [
    "def load_and_map_weights_for_sequence_model(model, checkpoint_path, config):\n",
    "    \"\"\"\n",
    "    .safetensors 체크포인트와 모델 간의 가중치를 정교하게 매핑하여 로드합니다.\n",
    "    \"\"\"\n",
    "    model_state_dict = model.state_dict()\n",
    "\n",
    "    state = {}\n",
    "    with safe_open(checkpoint_path, framework=\"pt\") as fp:\n",
    "        for k in fp.keys():\n",
    "            assert k not in state\n",
    "            state[k] = fp.get_tensor(k)\n",
    "\n",
    "    print(f'model tensors: ', model_state_dict.keys())\n",
    "    print('checkpoint tensors: ', state.keys())\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "\n",
    "    # 문제 5: 위에 출력한 모델 가중치의 키를 기반으로 새로운 가중치 딕셔너리를 만듭니다.\n",
    "    # [START CODE]\n",
    "\n",
    "    # 최상위 레벨 레이어 매핑\n",
    "    new_state_dict['tok_embedding.weight'] = state['model.embed_tokens.weight']\n",
    "    new_state_dict['final_norm.weight'] = state['model.norm.weight']\n",
    "\n",
    "    # score weight 을 로드합니다.\n",
    "    new_state_dict['??.??'] = state['??.??']\n",
    "\n",
    "    # Transformer 블록 내부 레이어 매핑\n",
    "    for i in range(model.config.num_layers):\n",
    "        new_state_dict[f'??.{i}.pre_atten_norm.weight'] = state[f'??.layers.{i}.input_layernorm.weight']\n",
    "        new_state_dict[f'??.{i}.post_atten_norm.weight'] = state[f'??.layers.{i}.post_attention_layernorm.weight']\n",
    "        new_state_dict[f'??.{i}.atten_func.output_projection.weight'] = state[f'??.layers.{i}.self_attn.o_proj.weight']\n",
    "        new_state_dict[f'??.{i}.ff.w1.weight'] = state[f'??.layers.{i}.mlp.gate_proj.weight']\n",
    "        new_state_dict[f'??.{i}.ff.w2.weight'] = state[f'??.layers.{i}.mlp.down_proj.weight']\n",
    "        new_state_dict[f'??.{i}.ff.w3.weight'] = state[f'??.layers.{i}.mlp.up_proj.weight']\n",
    "\n",
    "        q_weight = state[f'??.layers.{i}.self_attn.q_proj.weight']\n",
    "        k_weight = state[f'??.layers.{i}.self_attn.k_proj.weight']\n",
    "        v_weight = state[f'??.layers.{i}.self_attn.v_proj.weight']\n",
    "\n",
    "        attn_config = config.block_config(i).attn_config\n",
    "        if attn_config.qkv_fused_interleaved:\n",
    "            q_per_kv = attn_config.num_heads // attn_config.num_query_groups\n",
    "            qs = torch.split(q_weight, attn_config.head_dim * q_per_kv)\n",
    "            ks = torch.split(k_weight, attn_config.head_dim)\n",
    "            vs = torch.split(v_weight, attn_config.head_dim)\n",
    "            qkv_combined_weight = torch.cat([t for group in zip(qs, ks, vs) for t in group])\n",
    "        else:\n",
    "            qkv_combined_weight = torch.cat([q_weight, k_weight, v_weight], dim=0)\n",
    "        new_state_dict[f'??.{i}.atten_func.qkv_projection.weight'] = qkv_combined_weight\n",
    "    # [END CODE]\n",
    "\n",
    "    # 모델 가중치를 로드합니다.\n",
    "    model.load_state_dict(new_state_dict, strict=True)\n",
    "    print(\"safetensors 가중치를 성공적으로 매핑하여 모델에 로드했습니다!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgVqP7Iqd7WK"
   },
   "source": [
    "다음으로 구현한 함수를 통해 모델의 가중치를 불러오고 추론을 실행해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13529,
     "status": "ok",
     "timestamp": 1756038241395,
     "user": {
      "displayName": "isu jeong",
      "userId": "03509833375949211012"
     },
     "user_tz": -540
    },
    "id": "C5KvzBNUZOtQ",
    "outputId": "b47c3149-9879-4d71-fc4d-1d61c4ceab99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model tensors:  odict_keys(['tok_embedding.weight', 'transformer_blocks.0.pre_atten_norm.weight', 'transformer_blocks.0.atten_func.qkv_projection.weight', 'transformer_blocks.0.atten_func.output_projection.weight', 'transformer_blocks.0.post_atten_norm.weight', 'transformer_blocks.0.ff.w1.weight', 'transformer_blocks.0.ff.w2.weight', 'transformer_blocks.0.ff.w3.weight', 'transformer_blocks.1.pre_atten_norm.weight', 'transformer_blocks.1.atten_func.qkv_projection.weight', 'transformer_blocks.1.atten_func.output_projection.weight', 'transformer_blocks.1.post_atten_norm.weight', 'transformer_blocks.1.ff.w1.weight', 'transformer_blocks.1.ff.w2.weight', 'transformer_blocks.1.ff.w3.weight', 'transformer_blocks.2.pre_atten_norm.weight', 'transformer_blocks.2.atten_func.qkv_projection.weight', 'transformer_blocks.2.atten_func.output_projection.weight', 'transformer_blocks.2.post_atten_norm.weight', 'transformer_blocks.2.ff.w1.weight', 'transformer_blocks.2.ff.w2.weight', 'transformer_blocks.2.ff.w3.weight', 'transformer_blocks.3.pre_atten_norm.weight', 'transformer_blocks.3.atten_func.qkv_projection.weight', 'transformer_blocks.3.atten_func.output_projection.weight', 'transformer_blocks.3.post_atten_norm.weight', 'transformer_blocks.3.ff.w1.weight', 'transformer_blocks.3.ff.w2.weight', 'transformer_blocks.3.ff.w3.weight', 'transformer_blocks.4.pre_atten_norm.weight', 'transformer_blocks.4.atten_func.qkv_projection.weight', 'transformer_blocks.4.atten_func.output_projection.weight', 'transformer_blocks.4.post_atten_norm.weight', 'transformer_blocks.4.ff.w1.weight', 'transformer_blocks.4.ff.w2.weight', 'transformer_blocks.4.ff.w3.weight', 'transformer_blocks.5.pre_atten_norm.weight', 'transformer_blocks.5.atten_func.qkv_projection.weight', 'transformer_blocks.5.atten_func.output_projection.weight', 'transformer_blocks.5.post_atten_norm.weight', 'transformer_blocks.5.ff.w1.weight', 'transformer_blocks.5.ff.w2.weight', 'transformer_blocks.5.ff.w3.weight', 'transformer_blocks.6.pre_atten_norm.weight', 'transformer_blocks.6.atten_func.qkv_projection.weight', 'transformer_blocks.6.atten_func.output_projection.weight', 'transformer_blocks.6.post_atten_norm.weight', 'transformer_blocks.6.ff.w1.weight', 'transformer_blocks.6.ff.w2.weight', 'transformer_blocks.6.ff.w3.weight', 'transformer_blocks.7.pre_atten_norm.weight', 'transformer_blocks.7.atten_func.qkv_projection.weight', 'transformer_blocks.7.atten_func.output_projection.weight', 'transformer_blocks.7.post_atten_norm.weight', 'transformer_blocks.7.ff.w1.weight', 'transformer_blocks.7.ff.w2.weight', 'transformer_blocks.7.ff.w3.weight', 'transformer_blocks.8.pre_atten_norm.weight', 'transformer_blocks.8.atten_func.qkv_projection.weight', 'transformer_blocks.8.atten_func.output_projection.weight', 'transformer_blocks.8.post_atten_norm.weight', 'transformer_blocks.8.ff.w1.weight', 'transformer_blocks.8.ff.w2.weight', 'transformer_blocks.8.ff.w3.weight', 'transformer_blocks.9.pre_atten_norm.weight', 'transformer_blocks.9.atten_func.qkv_projection.weight', 'transformer_blocks.9.atten_func.output_projection.weight', 'transformer_blocks.9.post_atten_norm.weight', 'transformer_blocks.9.ff.w1.weight', 'transformer_blocks.9.ff.w2.weight', 'transformer_blocks.9.ff.w3.weight', 'transformer_blocks.10.pre_atten_norm.weight', 'transformer_blocks.10.atten_func.qkv_projection.weight', 'transformer_blocks.10.atten_func.output_projection.weight', 'transformer_blocks.10.post_atten_norm.weight', 'transformer_blocks.10.ff.w1.weight', 'transformer_blocks.10.ff.w2.weight', 'transformer_blocks.10.ff.w3.weight', 'transformer_blocks.11.pre_atten_norm.weight', 'transformer_blocks.11.atten_func.qkv_projection.weight', 'transformer_blocks.11.atten_func.output_projection.weight', 'transformer_blocks.11.post_atten_norm.weight', 'transformer_blocks.11.ff.w1.weight', 'transformer_blocks.11.ff.w2.weight', 'transformer_blocks.11.ff.w3.weight', 'transformer_blocks.12.pre_atten_norm.weight', 'transformer_blocks.12.atten_func.qkv_projection.weight', 'transformer_blocks.12.atten_func.output_projection.weight', 'transformer_blocks.12.post_atten_norm.weight', 'transformer_blocks.12.ff.w1.weight', 'transformer_blocks.12.ff.w2.weight', 'transformer_blocks.12.ff.w3.weight', 'transformer_blocks.13.pre_atten_norm.weight', 'transformer_blocks.13.atten_func.qkv_projection.weight', 'transformer_blocks.13.atten_func.output_projection.weight', 'transformer_blocks.13.post_atten_norm.weight', 'transformer_blocks.13.ff.w1.weight', 'transformer_blocks.13.ff.w2.weight', 'transformer_blocks.13.ff.w3.weight', 'transformer_blocks.14.pre_atten_norm.weight', 'transformer_blocks.14.atten_func.qkv_projection.weight', 'transformer_blocks.14.atten_func.output_projection.weight', 'transformer_blocks.14.post_atten_norm.weight', 'transformer_blocks.14.ff.w1.weight', 'transformer_blocks.14.ff.w2.weight', 'transformer_blocks.14.ff.w3.weight', 'transformer_blocks.15.pre_atten_norm.weight', 'transformer_blocks.15.atten_func.qkv_projection.weight', 'transformer_blocks.15.atten_func.output_projection.weight', 'transformer_blocks.15.post_atten_norm.weight', 'transformer_blocks.15.ff.w1.weight', 'transformer_blocks.15.ff.w2.weight', 'transformer_blocks.15.ff.w3.weight', 'transformer_blocks.16.pre_atten_norm.weight', 'transformer_blocks.16.atten_func.qkv_projection.weight', 'transformer_blocks.16.atten_func.output_projection.weight', 'transformer_blocks.16.post_atten_norm.weight', 'transformer_blocks.16.ff.w1.weight', 'transformer_blocks.16.ff.w2.weight', 'transformer_blocks.16.ff.w3.weight', 'transformer_blocks.17.pre_atten_norm.weight', 'transformer_blocks.17.atten_func.qkv_projection.weight', 'transformer_blocks.17.atten_func.output_projection.weight', 'transformer_blocks.17.post_atten_norm.weight', 'transformer_blocks.17.ff.w1.weight', 'transformer_blocks.17.ff.w2.weight', 'transformer_blocks.17.ff.w3.weight', 'transformer_blocks.18.pre_atten_norm.weight', 'transformer_blocks.18.atten_func.qkv_projection.weight', 'transformer_blocks.18.atten_func.output_projection.weight', 'transformer_blocks.18.post_atten_norm.weight', 'transformer_blocks.18.ff.w1.weight', 'transformer_blocks.18.ff.w2.weight', 'transformer_blocks.18.ff.w3.weight', 'transformer_blocks.19.pre_atten_norm.weight', 'transformer_blocks.19.atten_func.qkv_projection.weight', 'transformer_blocks.19.atten_func.output_projection.weight', 'transformer_blocks.19.post_atten_norm.weight', 'transformer_blocks.19.ff.w1.weight', 'transformer_blocks.19.ff.w2.weight', 'transformer_blocks.19.ff.w3.weight', 'transformer_blocks.20.pre_atten_norm.weight', 'transformer_blocks.20.atten_func.qkv_projection.weight', 'transformer_blocks.20.atten_func.output_projection.weight', 'transformer_blocks.20.post_atten_norm.weight', 'transformer_blocks.20.ff.w1.weight', 'transformer_blocks.20.ff.w2.weight', 'transformer_blocks.20.ff.w3.weight', 'transformer_blocks.21.pre_atten_norm.weight', 'transformer_blocks.21.atten_func.qkv_projection.weight', 'transformer_blocks.21.atten_func.output_projection.weight', 'transformer_blocks.21.post_atten_norm.weight', 'transformer_blocks.21.ff.w1.weight', 'transformer_blocks.21.ff.w2.weight', 'transformer_blocks.21.ff.w3.weight', 'transformer_blocks.22.pre_atten_norm.weight', 'transformer_blocks.22.atten_func.qkv_projection.weight', 'transformer_blocks.22.atten_func.output_projection.weight', 'transformer_blocks.22.post_atten_norm.weight', 'transformer_blocks.22.ff.w1.weight', 'transformer_blocks.22.ff.w2.weight', 'transformer_blocks.22.ff.w3.weight', 'transformer_blocks.23.pre_atten_norm.weight', 'transformer_blocks.23.atten_func.qkv_projection.weight', 'transformer_blocks.23.atten_func.output_projection.weight', 'transformer_blocks.23.post_atten_norm.weight', 'transformer_blocks.23.ff.w1.weight', 'transformer_blocks.23.ff.w2.weight', 'transformer_blocks.23.ff.w3.weight', 'transformer_blocks.24.pre_atten_norm.weight', 'transformer_blocks.24.atten_func.qkv_projection.weight', 'transformer_blocks.24.atten_func.output_projection.weight', 'transformer_blocks.24.post_atten_norm.weight', 'transformer_blocks.24.ff.w1.weight', 'transformer_blocks.24.ff.w2.weight', 'transformer_blocks.24.ff.w3.weight', 'transformer_blocks.25.pre_atten_norm.weight', 'transformer_blocks.25.atten_func.qkv_projection.weight', 'transformer_blocks.25.atten_func.output_projection.weight', 'transformer_blocks.25.post_atten_norm.weight', 'transformer_blocks.25.ff.w1.weight', 'transformer_blocks.25.ff.w2.weight', 'transformer_blocks.25.ff.w3.weight', 'transformer_blocks.26.pre_atten_norm.weight', 'transformer_blocks.26.atten_func.qkv_projection.weight', 'transformer_blocks.26.atten_func.output_projection.weight', 'transformer_blocks.26.post_atten_norm.weight', 'transformer_blocks.26.ff.w1.weight', 'transformer_blocks.26.ff.w2.weight', 'transformer_blocks.26.ff.w3.weight', 'transformer_blocks.27.pre_atten_norm.weight', 'transformer_blocks.27.atten_func.qkv_projection.weight', 'transformer_blocks.27.atten_func.output_projection.weight', 'transformer_blocks.27.post_atten_norm.weight', 'transformer_blocks.27.ff.w1.weight', 'transformer_blocks.27.ff.w2.weight', 'transformer_blocks.27.ff.w3.weight', 'transformer_blocks.28.pre_atten_norm.weight', 'transformer_blocks.28.atten_func.qkv_projection.weight', 'transformer_blocks.28.atten_func.output_projection.weight', 'transformer_blocks.28.post_atten_norm.weight', 'transformer_blocks.28.ff.w1.weight', 'transformer_blocks.28.ff.w2.weight', 'transformer_blocks.28.ff.w3.weight', 'transformer_blocks.29.pre_atten_norm.weight', 'transformer_blocks.29.atten_func.qkv_projection.weight', 'transformer_blocks.29.atten_func.output_projection.weight', 'transformer_blocks.29.post_atten_norm.weight', 'transformer_blocks.29.ff.w1.weight', 'transformer_blocks.29.ff.w2.weight', 'transformer_blocks.29.ff.w3.weight', 'final_norm.weight', 'score.weight'])\n",
      "checkpoint tensors:  dict_keys(['model.embed_tokens.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.norm.weight', 'score.weight'])\n",
      "safetensors 가중치를 성공적으로 매핑하여 모델에 로드했습니다!\n",
      "토크나이저 로드 완료.\n",
      "입력: 회원가입은 어떻게 하나요?\n",
      "예측: simple (tensor([-1.7978, -3.1667]))\n",
      "입력: 주문한 상품의 배송 상태를 추적하고 싶습니다.\n",
      "예측: complex (tensor([-1.3644, -1.3119]))\n",
      "입력: 결제 시 사용 가능한 할인 쿠폰이 있나요?\n",
      "예측: complex (tensor([-7.3173,  2.4323]))\n",
      "입력: 로그인 시도 시 오류 코드가 발생하는데 해결 방법은 무엇인가요?\n",
      "예측: complex (tensor([-8.8847,  2.3416]))\n",
      "입력: 주문 후 결제 수단을 변경할 수 있나요?\n",
      "예측: complex (tensor([-7.8300,  2.2063]))\n",
      "입력: 이 제품의 상세 스펙과 사용 후기를 알고 싶습니다.\n",
      "예측: complex (tensor([-2.7135, -2.1060]))\n"
     ]
    }
   ],
   "source": [
    "# 모델을 생성하고 가중치를 불러옵니다.\n",
    "config = get_model_config()\n",
    "model = build_model(config)\n",
    "loaded_model = load_and_map_weights_for_sequence_model(model,\n",
    "                                                       f'{smollm_checkpoint_path}/model.safetensors',\n",
    "                                                       config)\n",
    "loaded_model.eval() # 추론 모드로 설정\n",
    "\n",
    "# --- 1. 토크나이저 로드 ---\n",
    "# 사용자의 환경에 맞게 수정 필요: 사용자의 환경에 맞게 토크나이저 이름과 모델 경로를 수정하세요.\n",
    "tokenizer_path = smollm_checkpoint_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(smollm_checkpoint_path)\n",
    "print(\"토크나이저 로드 완료.\")\n",
    "\n",
    "# --- 2. 추론 실행 ---\n",
    "prompts = [\n",
    "    \"회원가입은 어떻게 하나요?\",\n",
    "    \"주문한 상품의 배송 상태를 추적하고 싶습니다.\",\n",
    "    \"결제 시 사용 가능한 할인 쿠폰이 있나요?\",\n",
    "    \"로그인 시도 시 오류 코드가 발생하는데 해결 방법은 무엇인가요?\",\n",
    "    \"주문 후 결제 수단을 변경할 수 있나요?\",\n",
    "    \"이 제품의 상세 스펙과 사용 후기를 알고 싶습니다.\",\n",
    "]\n",
    "\n",
    "# --- 3. 레이블 맵 설정 ---\n",
    "# 모델의 출력 인덱스(0, 1)를 실제 레이블 이름으로 매핑합니다.\n",
    "# 이 부분은 사용자가 학습시킨 분류 작업에 맞게 수정해야 합니다.\n",
    "tokenizer.id2label = {0: \"simple\", 1: \"complex\"}\n",
    "\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt,\n",
    "                        return_tensors='pt',\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        max_length=128)\n",
    "\n",
    "    # 문제 6: 위에 생성한 모델로 추론을 수행합니다.\n",
    "    # [START CODE]\n",
    "    result = ??\n",
    "    # [END CODE]\n",
    "\n",
    "    logits = result['logits'][0]\n",
    "    predicted_class_id = torch.argmax(logits).item()\n",
    "\n",
    "    print(f\"입력: {prompt}\")\n",
    "    print(f'예측: {tokenizer.id2label[predicted_class_id]} ({logits})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tr2yrR8jTmKm"
   },
   "source": [
    "다음으로 **PyTorch 모델을 TFLite로 변환하는 함수를 정의**합니다. 변환 과정은 다음과 같은 단계로 이루어집니다.\n",
    "\n",
    "1. PyTorch 모델 빌드: 사전 학습된 '몸통' 가중치를 포함한 분류 모델을 준비합니다. (미세 조정한 분류 헤드 가중치는 변환 직전에 별도로 로드합니다.)\n",
    "\n",
    "2. 더미 입력 생성: TFLite가 모델의 입출력 형태(shape)를 파악하고 그래프를 추적(trace)할 수 있도록, 실제 입력과 동일한 형태의 더미(dummy) 텐서를 생성합니다.\n",
    "\n",
    "3. 모델 변환: `ai_edge_torch.signature`로 모델의 서명(signature)을 정의하고, `.convert()` 메서드를 호출하여 TFLite 모델로 변환합니다.\n",
    "\n",
    "4. 파일로 내보내기: 변환된 모델을 `.tflite` 파일로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4LhJwbUThub"
   },
   "outputs": [],
   "source": [
    "def convert_classifier_to_tflite(\n",
    "    # 모델 빌드에 필요한 파라미터\n",
    "    pytorch_model: nn.Module, # 가중치가 모두 로드된 모델을 직접 받음\n",
    "    max_len: int = 128,\n",
    "    quantize_model: bool = True,\n",
    "    output_path: str = \"smollm_classifier.tflite\"\n",
    "):\n",
    "    \"\"\"\n",
    "    SequenceClassificationModel을 TFLite로 변환합니다.\n",
    "\n",
    "    Args:\n",
    "        pytorch_model (nn.Module): 가중치가 모두 로드된 PyTorch 모델.\n",
    "        max_len (int): TFLite 모델이 받을 최대 시퀀스 길이.\n",
    "        quantize_model (bool): 모델 양자화 여부.\n",
    "        output_path (str): 저장될 TFLite 파일의 경로.\n",
    "    \"\"\"\n",
    "    print(f\"'{output_path}' 파일로 TFLite 변환을 시작합니다...\")\n",
    "\n",
    "    # 1. 변환 중 모델 그래프를 추적(trace)하기 위한 더미 입력(dummy input)을 생성합니다.\n",
    "    #    TFLite 모델은 고정된 입력 크기를 가지므로, 배포 환경에서 사용할 최대 길이(max_len)로 설정합니다.\n",
    "    tokens = torch.zeros((1, max_len), dtype=torch.long)\n",
    "    attention_mask = torch.ones((1, max_len), dtype=torch.long)\n",
    "\n",
    "    # 2. (선택 사항) 모델의 양자화(quantization) 설정을 정의합니다.\n",
    "    #    양자화는 모델의 가중치를 float32에서 int8 등으로 낮춰 크기를 줄이고 추론 속도를 높이는 기술입니다.\n",
    "    quant_config = quant_recipes.full_int8_dynamic_recipe() if quantize_model else None\n",
    "\n",
    "    # 3. ai_edge_torch를 사용하여 모델을 변환합니다.\n",
    "    edge_model = (\n",
    "        ai_edge_torch.signature(\n",
    "            \"serving_default\",\n",
    "            pytorch_model,\n",
    "            # 문제 7: 그래프를 추적하기 위해 더미 입력을 입력합니다.\n",
    "            # 모델과 함께 더미 입력을 전달하여 그래프를 추적합니다.\n",
    "            # [START CODE]\n",
    "            (??, ??)\n",
    "            # [END CODE]\n",
    "        )\n",
    "        # .convert() 메서드가 실제 변환을 수행합니다.\n",
    "        .convert(quant_config=quant_config)\n",
    "    )\n",
    "\n",
    "    # 4. TFLite 파일로 내보냅니다.\n",
    "    edge_model.export(output_path)\n",
    "    print(f\"성공적으로 변환하여 '{output_path}'에 저장했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7muJi4dIUWkd"
   },
   "source": [
    "이제 모든 준비가 끝났습니다. 다음 단계를 거쳐 실제 모델 변환을 실행합니다.\n",
    "\n",
    "1. 사전 학습된 가중치가 있는 체크포인트 경로를 설정합니다.\n",
    "2. `build_model_v2` 함수를 사용해 트랜스포머 몸통 부분의 가중치가 로드된 모델을 생성합니다.\n",
    "3. 분류 작업에 맞게 미세 조정한 score 레이어의 가중치를 불러와 모델에 덮어씌웁니다.\n",
    "4. `convert_classifier_to_tflite` 함수를 호출하여 최종적으로 TFLite 모델을 생성하고 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 385251,
     "status": "ok",
     "timestamp": 1756035338326,
     "user": {
      "displayName": "isu jeong",
      "userId": "03509833375949211012"
     },
     "user_tz": -540
    },
    "id": "cZdSFhzbUP7X",
    "outputId": "2c63b306-f69c-4392-d5b0-ea182521351d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/content/drive/MyDrive/Upstage-AI/module-14/smollm2_135m_classifier_f32.tflite' 파일로 TFLite 변환을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성공적으로 변환하여 '/content/drive/MyDrive/Upstage-AI/module-14/smollm2_135m_classifier_f32.tflite'에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "# --- TFLite 변환 실행 ---\n",
    "\n",
    "output_f32_tflite_path = f\"{root}/module-14/smollm2_135m_classifier_f32.tflite\"\n",
    "\n",
    "# 모델을 TFLite로 변환합니다.\n",
    "convert_classifier_to_tflite(\n",
    "    pytorch_model=loaded_model,\n",
    "    max_len=128,                # TFLite 모델의 최대 입력 길이\n",
    "    quantize_model=False,       # 양자화 비활성화 (float32 모델 생성)\n",
    "    output_path=output_f32_tflite_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 395497,
     "status": "ok",
     "timestamp": 1756035733820,
     "user": {
      "displayName": "isu jeong",
      "userId": "03509833375949211012"
     },
     "user_tz": -540
    },
    "id": "eMO7dnFKXhbV",
    "outputId": "49862404-8d06-47d2-cacf-82ea30bb62e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/content/drive/MyDrive/Upstage-AI/module-14/smollm2_135m_classifier_i8.tflite' 파일로 TFLite 변환을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/export/_unlift.py:75: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/fx/graph.py:1801: UserWarning: Node mask_cache target mask_cache mask_cache of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성공적으로 변환하여 '/content/drive/MyDrive/Upstage-AI/module-14/smollm2_135m_classifier_i8.tflite'에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "output_i8_tflite_path = f\"{root}/module-14/smollm2_135m_classifier_i8.tflite\"\n",
    "\n",
    "# 모델을 TFLite로 변환합니다.\n",
    "convert_classifier_to_tflite(\n",
    "    pytorch_model=loaded_model,\n",
    "    max_len=128,                # TFLite 모델의 최대 입력 길이\n",
    "    quantize_model=True,       # 양자화 비활성화 (float32 모델 생성)\n",
    "    output_path=output_i8_tflite_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1756035733822,
     "user": {
      "displayName": "isu jeong",
      "userId": "03509833375949211012"
     },
     "user_tz": -540
    },
    "id": "Y15Dr4EztUlh",
    "outputId": "493da298-adb3-4c84-9375-e3b9616bc56d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 크기 (MB): 530.02 MB\n",
      "파일 크기 (MB): 147.61 MB\n"
     ]
    }
   ],
   "source": [
    "# 저장된 tflite 파일들의 크기를 확인해봅니다.\n",
    "\n",
    "size_in_bytes = os.path.getsize(output_f32_tflite_path)\n",
    "size_in_mb = size_in_bytes / (1024 * 1024)\n",
    "print(f\"파일 크기 (MB): {size_in_mb:.2f} MB\")\n",
    "\n",
    "size_in_bytes = os.path.getsize(output_i8_tflite_path)\n",
    "size_in_mb = size_in_bytes / (1024 * 1024)\n",
    "print(f\"파일 크기 (MB): {size_in_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lErfgjKxppYu"
   },
   "source": [
    "# **실습2: 변환된 모델을 tflite interpreter 을 통해 실행**\n",
    "\n",
    "이 파트에서는 이전에 PyTorch 모델에서 변환한 .tflite 파일을 불러와, 텍스트에 대한 분류를 수행하는 전체 과정을 학습합니다.\n",
    "\n",
    "TensorFlow Lite Interpreter를 사용하여 모델을 로드하고, 입력 데이터를 전처리하여 추론을 실행한 뒤, 최종 결과를 해석하는 방법을 단계별로 알아봅니다.\n",
    "\n",
    "</br>\n",
    "\n",
    "**학습 목표**:\n",
    "  - TensorFlow Lite Interpreter를 사용하여 TFLite 모델을 로드하는 방법을 이해합니다.\n",
    "  - 모델 추론을 위한 입력 데이터 전처리 과정을 학습합니다.\n",
    "  - TFLite 모델의 signature_runner를 사용하여 추론을 실행하는 방법을 파악합니다.\n",
    "  - 모델의 출력(logits)을 해석하여 최종 예측 레이블과 신뢰도 점수를 얻는 방법을 이해합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr4m3J2vpwkW"
   },
   "source": [
    "모델을 실행하고 데이터를 처리하는 데 필요한 **라이브러리들을 임포트**합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwEAmNYabxqZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import lite\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egCej_6UqvCN"
   },
   "source": [
    "TFLite 모델을 더 편리하게 사용하기 위해, 모델 로딩, 데이터 전처리, 추론, 후처리 과정을 하나로 묶는 래퍼(wrapper) 클래스 **LiteRTClassifier를 정의**합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqSJ4hZ3qxbX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import lite\n",
    "\n",
    "class LiteRTClassifier:\n",
    "    \"\"\"TFLite 시퀀스 분류 모델을 위한 래퍼(wrapper) 클래스.\"\"\"\n",
    "\n",
    "    def __init__(self, tfl_path: str, tokenizer, max_len: int = 128):\n",
    "        \"\"\"\n",
    "        클래스를 초기화하고 TFLite 모델과 토크나이저를 로드합니다.\n",
    "\n",
    "        Args:\n",
    "            tfl_path (str): .tflite 모델 파일의 경로.\n",
    "            tokenizer: Hugging Face의 사전 학습된 토크나이저.\n",
    "            max_len (int): 모델이 처리할 수 있는 최대 시퀀스 길이.\n",
    "        \"\"\"\n",
    "        # --- 1. 토크나이저 및 모델 설정 초기화 ---\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # --- 2. TFLite 모델 로드 ---\n",
    "        self.interp = lite.Interpreter(model_path=tfl_path)\n",
    "        self.runner = self.interp.get_signature_runner('serving_default')\n",
    "\n",
    "        # --- 3. 모델의 입력 이름 동적 확인 ---\n",
    "        # 하드코딩 대신 모델 시그니처에서 직접 입력 키 이름을 가져옵니다.\n",
    "        # 이렇게 하면 모델 변환 시 이름이 'args_0', 'args_1' 등으로 바뀌어도 코드를 수정할 필요가 없습니다.\n",
    "        input_details = self.runner.get_input_details()\n",
    "        self.input_keys = list(input_details.keys())\n",
    "\n",
    "        # 모델이 두 개의 입력을 받는지 확인합니다.\n",
    "        if len(self.input_keys) != 2:\n",
    "            raise ValueError(\n",
    "                f\"TFLite 모델이 2개의 입력을 받을 것으로 예상했지만, {len(self.input_keys)}개를 받습니다. \"\n",
    "                f\"입력 이름: {self.input_keys}\"\n",
    "            )\n",
    "\n",
    "    def predict(self, text: str):\n",
    "        \"\"\"주어진 텍스트를 분류합니다.\"\"\"\n",
    "\n",
    "        # --- 단계 1: 입력 데이터 전처리 (Preprocessing) ---\n",
    "        # 토크나이저는 'input_ids'와 'attention_mask'를 모두 반환합니다.\n",
    "        inputs = self.tok(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"np\"\n",
    "        )\n",
    "\n",
    "        # TFLite 모델의 입력 타입(int64)으로 변환합니다.\n",
    "        tokens = inputs[\"input_ids\"].astype(\"int64\")\n",
    "        attention_mask = inputs[\"attention_mask\"].astype(\"int64\")\n",
    "\n",
    "        # --- 단계 2: 모델 입력 텐서 매핑 ---\n",
    "        # 동적으로 찾은 입력 이름에 맞게 tokens와 attention_mask를 매핑합니다.\n",
    "        # 첫 번째 입력은 tokens, 두 번째는 attention_mask로 가정합니다.\n",
    "        # 문제 8: 모델의 입력에 맞게 텐서를 매핑시킵니다.\n",
    "        # [START CODE]\n",
    "        feed_dict = {\n",
    "            ??: ??,\n",
    "            ??: ??\n",
    "        }\n",
    "        # [END CODE]\n",
    "\n",
    "        # --- 단계 3: 추론 실행 (Inference) ---\n",
    "        outputs = self.runner(**feed_dict)\n",
    "        logits = outputs[\"logits\"]\n",
    "\n",
    "        # --- 단계 4: 결과 후처리 (Post-processing) ---\n",
    "        predicted_class_id = int(np.argmax(logits, axis=-1)[0])\n",
    "        score = float(logits[0, predicted_class_id])\n",
    "        label = self.tok.id2label[predicted_class_id]\n",
    "\n",
    "        return {\n",
    "            \"label\": label,\n",
    "            \"score\": score,\n",
    "            'predicted_class_id': predicted_class_id,\n",
    "            'logits': logits\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUJRBFFNq7Ip"
   },
   "source": [
    "이제 LiteRTClassifier 클래스를 사용하여 실제 **텍스트에 대한 분류를 수행**해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1756035762615,
     "user": {
      "displayName": "isu jeong",
      "userId": "03509833375949211012"
     },
     "user_tz": -540
    },
    "id": "qtOXpqnXtjIv",
    "outputId": "978ab983-1e64-44bd-f6be-7d278e919201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류기 생성 완료.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---  분류기 클래스 인스턴스화 ---\n",
    "# 사용자의 환경에 맞게 수정 필요: 변환된 tflite 파일의 경로를 입력하세요\n",
    "output_fp32_tflite_path = f\"{root}/module-14/smollm2_135m_classifier_f32.tflite\"\n",
    "output_i8_tflite_path = f\"{root}//module-14/smollm2_135m_classifier_i8.tflite\"\n",
    "\n",
    "classifier_fp32 = LiteRTClassifier(output_fp32_tflite_path, tokenizer, max_len=128)\n",
    "classifier_i8 = LiteRTClassifier(output_i8_tflite_path, tokenizer, max_len=128)\n",
    "print(\"분류기 생성 완료.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10181,
     "status": "ok",
     "timestamp": 1756035773426,
     "user": {
      "displayName": "isu jeong",
      "userId": "03509833375949211012"
     },
     "user_tz": -540
    },
    "id": "u1DKpUl-q45P",
    "outputId": "465e46ac-181d-42b1-b691-756b4875ca5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------float 32 model 의 예측을 수행합니다.------\n",
      "입력: '회원가입은 어떻게 하나요?'\n",
      "-> 예측 결과: '[[-1.7978024 -3.1666348]]' 'simple' (신뢰도: -1.7978)\n",
      "   추론 시간: 2.1719 초\n",
      "\n",
      "입력: '주문한 상품의 배송 상태를 추적하고 싶습니다.'\n",
      "-> 예측 결과: '[[-1.36443   -1.3118492]]' 'complex' (신뢰도: -1.3118)\n",
      "   추론 시간: 0.8846 초\n",
      "\n",
      "입력: '결제 시 사용 가능한 할인 쿠폰이 있나요?'\n",
      "-> 예측 결과: '[[-7.3172684  2.432322 ]]' 'complex' (신뢰도: 2.4323)\n",
      "   추론 시간: 0.8422 초\n",
      "\n",
      "입력: '로그인 시도 시 오류 코드가 발생하는데 해결 방법은 무엇인가요?'\n",
      "-> 예측 결과: '[[-8.884714   2.3416142]]' 'complex' (신뢰도: 2.3416)\n",
      "   추론 시간: 0.5167 초\n",
      "\n",
      "입력: '주문 후 결제 수단을 변경할 수 있나요?'\n",
      "-> 예측 결과: '[[-7.830002   2.2063122]]' 'complex' (신뢰도: 2.2063)\n",
      "   추론 시간: 0.5237 초\n",
      "\n",
      "입력: '이 제품의 상세 스펙과 사용 후기를 알고 싶습니다.'\n",
      "-> 예측 결과: '[[-2.7135189 -2.105986 ]]' 'complex' (신뢰도: -2.1060)\n",
      "   추론 시간: 0.4935 초\n",
      "\n",
      "\n",
      "------int 8 model 의 예측을 수행합니다.------\n",
      "입력: '회원가입은 어떻게 하나요?'\n",
      "-> 예측 결과: '[[-1.9057783 -3.1054816]]' 'simple' (신뢰도: -1.9058)\n",
      "   추론 시간: 0.9544 초\n",
      "\n",
      "입력: '주문한 상품의 배송 상태를 추적하고 싶습니다.'\n",
      "-> 예측 결과: '[[-1.666312  -2.1046126]]' 'simple' (신뢰도: -1.6663)\n",
      "   추론 시간: 0.8249 초\n",
      "\n",
      "입력: '결제 시 사용 가능한 할인 쿠폰이 있나요?'\n",
      "-> 예측 결과: '[[-6.7463427  2.0188093]]' 'complex' (신뢰도: 2.0188)\n",
      "   추론 시간: 0.8540 초\n",
      "\n",
      "입력: '로그인 시도 시 오류 코드가 발생하는데 해결 방법은 무엇인가요?'\n",
      "-> 예측 결과: '[[-8.326611   1.7817827]]' 'complex' (신뢰도: 1.7818)\n",
      "   추론 시간: 0.8317 초\n",
      "\n",
      "입력: '주문 후 결제 수단을 변경할 수 있나요?'\n",
      "-> 예측 결과: '[[-7.629795   1.6718655]]' 'complex' (신뢰도: 1.6719)\n",
      "   추론 시간: 0.7359 초\n",
      "\n",
      "입력: '이 제품의 상세 스펙과 사용 후기를 알고 싶습니다.'\n",
      "-> 예측 결과: '[[-2.5468893 -2.6988063]]' 'simple' (신뢰도: -2.5469)\n",
      "   추론 시간: 0.5346 초\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 추론 실행 ---\n",
    "prompts = [\n",
    "    \"회원가입은 어떻게 하나요?\",\n",
    "    \"주문한 상품의 배송 상태를 추적하고 싶습니다.\",\n",
    "    \"결제 시 사용 가능한 할인 쿠폰이 있나요?\",\n",
    "    \"로그인 시도 시 오류 코드가 발생하는데 해결 방법은 무엇인가요?\",\n",
    "    \"주문 후 결제 수단을 변경할 수 있나요?\",\n",
    "    \"이 제품의 상세 스펙과 사용 후기를 알고 싶습니다.\",\n",
    "]\n",
    "\n",
    "print(\"\\n------float 32 model 의 예측을 수행합니다.------\")\n",
    "for prompt in prompts:\n",
    "    start_time = time.time()\n",
    "    result = classifier_fp32.predict(prompt)\n",
    "    end_time = time.time()\n",
    "\n",
    "    inference_time = end_time - start_time\n",
    "\n",
    "    print(f\"입력: '{prompt}'\")\n",
    "    print(f\"-> 예측 결과: '{result['logits']}' '{result['label']}' (신뢰도: {result['score']:.4f})\")\n",
    "    print(f\"   추론 시간: {inference_time:.4f} 초\\n\")\n",
    "\n",
    "print(\"\\n------int 8 model 의 예측을 수행합니다.------\")\n",
    "for prompt in prompts:\n",
    "    start_time = time.time()\n",
    "    result = classifier_i8.predict(prompt)\n",
    "    end_time = time.time()\n",
    "\n",
    "    inference_time = end_time - start_time\n",
    "\n",
    "    print(f\"입력: '{prompt}'\")\n",
    "    print(f\"-> 예측 결과: '{result['logits']}' '{result['label']}' (신뢰도: {result['score']:.4f})\")\n",
    "    print(f\"   추론 시간: {inference_time:.4f} 초\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RYdI8xruPNG"
   },
   "source": [
    "Colab 환경의 일반 CPU에서 테스트했을 때 float32와 int8 모델의 속도 차이가 기대만큼 드라마틱하게 차이나지 않는데요. 이는 자연스러운 현상입니다.\n",
    "\n",
    "이는 int8 연산의 진짜 힘이 NPU, TPU, 모바일 GPU와 같은 특화된 하드웨어 가속기에서 발휘되기 때문입니다.\n",
    "\n",
    "CPU 는 소수의 매우 복잡하고 강력한 코어를 가지고 있습니다. 각 코어는 어려운 문제를 순서대로 처리하는 데 특화되어 있으며, 특히 소수점 연산(Float32)을 위한 전용 회로(FPU, Floating-Point Unit)가 고도로 발달해 있습니다.\n",
    "\n",
    "반대로  NPU/TPU는 수천, 수만 개의 매우 단순하고 작은 연산 유닛(MAC, Multiply-Accumulate)으로 구성되어 있습니다. 이 유닛들은 복잡한 작업은 못 하지만, 단순한 정수(INT8) 곱셈과 덧셈을 대규모로 동시에(in parallel) 처리하는 능력은 뛰어납니다.\n",
    "\n",
    "Colab의 CPU는 이미 Float32 연산에 매우 뛰어나기 때문에, 그보다 단순한 INT8 연산을 처리해도 속도 향상 폭이 크지 않습니다. 하지만 스마트폰에 탑재된 NPU는 설계 단계부터 수많은 코어가 INT8 연산을 동시에 처리하도록 만들어졌기 때문에, INT8로 양자화된 모델을 만나면 하드웨어의 잠재력이 증가하여 극적인 속도 향상과 함께 전력 소모량도 크게 줄일 수 있을 것 입니다.\n",
    "\n",
    "결론적으로 Colab 환경에서 INT8 모델의 주된 이점은 획기적으로 줄어든 모델 파일 크기와 감소된 메모리 사용량입니다. 약간의 속도 향상은 덤이라고 생각할 수 있습니다. 드라마틱한 속도 향상은 실제 타겟 디바이스(스마트폰 등)에 배포했을 때 비로소 체감할 수 있습니다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
