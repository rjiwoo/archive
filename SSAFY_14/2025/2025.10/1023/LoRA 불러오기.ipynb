{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 5-12. 저장된 모델 불러오기\n",
        "\n",
        "힘들게 학습시킨 모델을 불러오겠습니다.\n",
        "- 제대로 불러오는지 확인하기 위해, [런타임 연결 해제 및 삭제] 한다.\n",
        "- Runtime을 새로 연결해주세요.\n",
        "- 필요한 Library를 처음부터 다시 설치할겁니다."
      ],
      "metadata": {
        "id": "YrovZUcPbhix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "   # Colab은 아래 코드가 수행됩니다.\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "ykfjWAhwblKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전에 저장했던 'gemma_jeju_lora' 폴더를 Colab에 업로드 합니다.\n",
        "- 'gemma_jeju_lora' 폴더 생성하기\n",
        "- 생성한 폴더에 이전에 저장했던 파일들을 끌어다 놓기"
      ],
      "metadata": {
        "id": "rdN_33mWbooY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **LoRA 어댑터만 저장한 경우**: 아래 명령어를 실행해 불러옵니다."
      ],
      "metadata": {
        "id": "_Dh0GxOfbuJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Colab 기준 2분 소요\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"gemma_jeju_lora\",  # 저장한 LoRA 어댑터 경로\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")"
      ],
      "metadata": {
        "id": "IeUPgZNwbwmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "불러온 모델을 이용해 추론해봅시다."
      ],
      "metadata": {
        "id": "1iMikif7b0yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"다음 문장을 제주도 사투리로 번역해줘:\\n\\n 귤 먹으면서 바다에서 놀면 좋아요!\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 1000, # Increase for longer outputs!\n",
        "    temperature = 1.4, top_p = 0.9, top_k = 20, # For non thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ],
      "metadata": {
        "id": "dLwXgoEnb5O_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}